{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "metadata": {
    "interpreter": {
     "hash": "dca0ade3e726a953b501b15e8e990130d2b7799f14cfd9f4271676035ebe5511"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## wifi feature by kuto\n",
    "\n",
    "[yukiさんのdataset](https://www.kaggle.com/dataset/951fe0fd675e58937311e936e941b01d517c227ce30192c8477bbe2bddeec602)をもとにを参考に、waypointを補正したwifi featureを作成する。  \n",
    "trainにwifi_x, wifi_yが加わっただけ。  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '..'\n",
    "feature_dir = f\"{base_path}/input/wifi-feature-with-timestamp\"\n",
    "train_files = sorted(glob.glob(os.path.join(feature_dir, '*_train.csv')))\n",
    "test_files = sorted(glob.glob(os.path.join(feature_dir, '*_test.csv')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file = train_files[0]\n",
    "train = pd.read_csv(train_file, index_col=0).reset_index(drop=True).rename(columns={'foor_str':'floor_str'})\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys \n",
    "sys.path.append(\"../\")\n",
    "from multiprocessing import Pool\n",
    "\n",
    "# io_f, compute_fはコンペのgithubから持ってきたファイル\n",
    "from src.io_f import read_data_file\n",
    "import src.compute_f as compute_f\n",
    "\n",
    "# cost minimizationから引用\n",
    "def compute_rel_positions(acce_datas, ahrs_datas):\n",
    "    step_timestamps, step_indexs, step_acce_max_mins = compute_f.compute_steps(acce_datas)\n",
    "    headings = compute_f.compute_headings(ahrs_datas)\n",
    "    stride_lengths = compute_f.compute_stride_length(step_acce_max_mins)\n",
    "    step_headings = compute_f.compute_step_heading(step_timestamps, headings)\n",
    "    rel_positions = compute_f.compute_rel_positions(stride_lengths, step_headings)\n",
    "    return rel_positions\n",
    "\n",
    "\n",
    "def complement_sensor_waypoint(trajectory_timestamp, trajectory_waypoint, sensor):\n",
    "    sensor_timestamp = sensor[:, 0]\n",
    "    sensor_rel_waypoint = sensor[:, 1:]\n",
    "    sensor_waypoint = []\n",
    "    for i in range(len(trajectory_timestamp)-1):\n",
    "        # あるwaypointから次のwaypointの間にあるsensorデータを取り出したいのでtimestampで該当のidxを取得\n",
    "        if i == len(trajectory_timestamp)-2:\n",
    "            # 最後のwaypointより未来のセンサデータがあるようなのでこの処理を追加\n",
    "            target_idx = (sensor_timestamp >= trajectory_timestamp[i])\n",
    "        else:\n",
    "            target_idx = (sensor_timestamp >= trajectory_timestamp[i]) & (sensor_timestamp < trajectory_timestamp[i+1])\n",
    "        # 対象区間の相対位置の累積和を取りスタート地点のwaypointを足すことでsensorによる位置が取得できる\n",
    "        tmp_sensor_waypoint = trajectory_waypoint[i] + np.cumsum(sensor_rel_waypoint[target_idx], axis=0)\n",
    "        sensor_waypoint.append(tmp_sensor_waypoint)\n",
    "    sensor_waypoint = np.concatenate(sensor_waypoint)\n",
    "    return sensor_timestamp, sensor_waypoint\n",
    "\n",
    "\n",
    "# wifiのtimestampに最も近いものをsensor_timestampから取得しそれに対応するwaypointをwifiのwaypointとして取得\n",
    "def complement_wifi_waypoint(wifi_timestamp, sensor_timestamp, sensor_waypoint):\n",
    "    x_sensor = sensor_waypoint[:,0]\n",
    "    y_sensor = sensor_waypoint[:,1]\n",
    "    wifi_x_by_sensor = []\n",
    "    wifi_y_by_sensor = []\n",
    "    wifi_timestamp_by_sensor = []\n",
    "    for i in wifi_timestamp:\n",
    "        idx = np.abs(sensor_timestamp - i).argmin()  # wifiデータとtimestampが最も近いものをsensor_timestampから取得\n",
    "        wifi_timestamp_by_sensor.append(sensor_timestamp[idx])\n",
    "        wifi_x_by_sensor.append(x_sensor[idx])\n",
    "        wifi_y_by_sensor.append(y_sensor[idx])\n",
    "\n",
    "    wifi_waypoint_by_sensor = np.stack([wifi_x_by_sensor, wifi_y_by_sensor], axis=1)\n",
    "    return wifi_waypoint_by_sensor\n",
    "\n",
    "\n",
    "def get_wifi_waypoint(site, floor, path, timestamp):\n",
    "    path_file = f'../input/indoor-location-navigation/train/{site}/{floor}/{path}.txt'\n",
    "    example = read_data_file(path_file)\n",
    "    trajectory = example.waypoint\n",
    "    wifi = example.wifi\n",
    "    sensor = compute_rel_positions(example.acce, example.ahrs)\n",
    "\n",
    "    trajectory_timestamp = trajectory[:,0]\n",
    "    trajectory_waypoint = trajectory[:, 1:]\n",
    "    wifi_timestamp = np.unique(wifi[:, 0]).astype(int)\n",
    "    sensor_timestamp, sensor_waypoint = complement_sensor_waypoint(trajectory_timestamp, trajectory_waypoint, sensor)\n",
    "    wifi_waypoint = complement_wifi_waypoint(wifi_timestamp, sensor_timestamp, sensor_waypoint)\n",
    "\n",
    "    return wifi_waypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def make_dataset_for_train(train_file):\n",
    "    train = pd.read_csv(train_file, index_col=0).reset_index(drop=True).rename(columns={'foor_str':'floor_str'})\n",
    "    file_name = train_file.split('/')[-1]\n",
    "    num_of_lines = train.shape[0]\n",
    "    print(f'{file_name} : {num_of_lines}')\n",
    "\n",
    "    data_list = []\n",
    "    for path, df in tqdm(train.groupby('path')):\n",
    "\n",
    "        bssid = df.columns[:-7].values\n",
    "        rssi = df.iloc[:, :-7].values\n",
    "        targets = df.iloc[:,-7:].values\n",
    "\n",
    "        # path内の各waypointのwifi値を取得\n",
    "        sort_rssi = []\n",
    "        sort_bssid = []\n",
    "        for i in range(len(df)):\n",
    "            sort_rssi.append(np.sort(rssi[i])[::-1][:100])\n",
    "            idx = np.argsort(rssi[i])[::-1]\n",
    "            sort_bssid.append(bssid[idx][:100])\n",
    "\n",
    "        sort_rssi = np.stack(sort_rssi)\n",
    "        sort_bssid = np.stack(sort_bssid)\n",
    "        site = train_file.split('/')[-1].split('_')[0]\n",
    "        timestamp = targets[i][0]\n",
    "  \n",
    "        wifi_waypoint = get_wifi_waypoint(site, floor, path, timestamp)  # 1行ずつ処理するのは計算コスト的にもったいない\n",
    "        path_data = np.concatenate((sort_bssid, sort_rssi, targets, wifi_waypoint), axis=1)\n",
    "        path_df = pd.DataFrame(path_data)\n",
    "        data_list.append(path_df)\n",
    "    \n",
    "    data_df = pd.concat(data_list)\n",
    "    columns = [f'bssid_{str(i)}' for i in range(100)] + [f'rssi_{str(i)}' for i in range(100)] + \\\n",
    "            ['timestamp', 'x', 'y', 'floor', 'floor_str', 'path', 'time_diff', 'wifi_x', 'wifi_y']\n",
    "    data_df.columns = columns\n",
    "    data_df.to_csv(f'../input/kuto_wifi_dataset_v1/{file_name}', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with Pool(processes=24) as pool:\n",
    "    pool.map(make_dataset_for_train, train_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# このsiteだけ失敗したので再度実行(以下のファイルのwaypointの改行がズレている)\n",
    "# indoor-location-navigation/train/5c3c44b80379370013e0fd2b/F1/5d077e040e86b60008036270.txt\n",
    "# indoor-location-navigation/train/5c3c44b80379370013e0fd2b/F2/5d0795110e86b600080363bc.txt\n",
    "make_dataset_for_train('../input/wifi-feature-with-timestamp/5c3c44b80379370013e0fd2b_timediff_1000_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset_for_test(test_file):\n",
    "    train = pd.read_csv(test_file)\n",
    "\n",
    "    file_name = test_file.split('/')[-1]\n",
    "\n",
    "    num_of_lines = train.shape[0]\n",
    "    print(f'{file_name} : {num_of_lines}')\n",
    "\n",
    "    data = None\n",
    "    for i in tqdm(range(num_of_lines)):\n",
    "\n",
    "        tmp = train.iloc[i,1:-2].astype(int).sort_values(ascending=False).head(100)\n",
    "        target = train.iloc[i, -2:]\n",
    "\n",
    "        line = pd.concat([pd.Series(tmp.index.astype(str)), tmp.astype(int), pd.Series(target)])\n",
    "        line.index = [str(i) for i in range(202)]\n",
    "        if data is None:\n",
    "            data = pd.DataFrame(line).T\n",
    "            data.columns = [str(i) for i in range(202)]\n",
    "        else:\n",
    "            data = data.append(line, ignore_index=True)\n",
    "    data.columns = [f'bssid_{str(i)}' for i in range(100)] + [f'rssi_{str(i)}' for i in range(100)] + ['site_path_timestamp', 'time_diff']\n",
    "    data.to_csv(f'../input/kuto_wifi_dataset_v1/{file_name}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Pool(processes=24) as pool:\n",
    "    pool.map(make_dataset_for_test, test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "dfs = []\n",
    "for f in glob.glob('../input/kuto_wifi_dataset_v1/*train.csv'):\n",
    "    site_id = f.split('/')[-1].split('_')[0]\n",
    "    _df = pd.read_csv(f)\n",
    "    _df['site_id'] = site_id\n",
    "    dfs.append(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_pickle(filename, obj):\n",
    "    with open(filename, mode='wb') as f:\n",
    "        pickle.dump(obj, f)\n",
    "        \n",
    "to_pickle('../input/kuto_wifi_dataset_v1/train_all.pkl', pd.concat(dfs).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "for f in glob.glob('../input/kuto_wifi_dataset_v1/*test.csv'):\n",
    "    site_id = f.split('/')[-1].split('_')[0]\n",
    "    _df = pd.read_csv(f)\n",
    "    _df['site_id'] = site_id\n",
    "    dfs.append(_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_pickle('../input/kuto_wifi_dataset_v1/test_all.pkl', pd.concat(dfs).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}