{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import yaml\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.utilities.seed import seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "with open('config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# globals variable\n",
    "SEED = config['globals']['seed']\n",
    "MAX_EPOCHS = config['globals']['max_epochs']\n",
    "N_SPLITS = config['globals']['n_splits']\n",
    "USE_FOLDS = config['globals']['use_folds']\n",
    "DEBUG = config['globals']['debug']\n",
    "EXP_MESSAGE = config['globals']['exp_message']\n",
    "NOTES = config['globals']['notes']\n",
    "MODEL_SAVE = config['globals']['model_save']\n",
    "PRETRAINED = config['globals']['pretrained']\n",
    "PRETRAINED_PATH = config['globals']['pretrained_path']\n",
    "EXP_NAME = str(Path().resolve()).split('/')[-1]\n",
    "\n",
    "# seed\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login 1bb2d0449c11d8b987e25c38b9d8dda176310fb6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waypointを補正したdataset\n",
    "root_dir = Path('../../input/')\n",
    "with open(root_dir/'kuto_wifi_dataset_v2/train_all.pkl', 'rb') as f:\n",
    "  train_df = pickle.load(f)\n",
    "\n",
    "with open(root_dir/'kuto_wifi_dataset_v2/test_all.pkl', 'rb') as f:\n",
    "  test_df = pickle.load(f)\n",
    "\n",
    "sub = pd.read_csv(root_dir/'indoor-location-navigation/sample_submission.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['x'].nunique(), train_df['wifi_x'].nunique()"
   ]
  },
  {
   "source": [
    "## time_diffの前処理\n",
    "0~3000sのものはwifi_x,wifi_yを使用  \n",
    "-1000s~0sのものはもともとのx.yを使用"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POSI_DIFF = 4000  # i番目のwaypointを基準に算出したwifi waypointのうち基準のtimestampの直近3sを信頼できるデータとして残す\n",
    "# NEGA_DIFF = -100000  # i番目のwaypointを基準に算出したwifi waypointのうちi+1番目のwaypointに近いものにはi+1のwaypointを座標として与える\n",
    "\n",
    "# train_df.loc[(NEGA_DIFF < train_df['time_diff']) & (train_df['time_diff'] <= 0), 'wifi_x'] = train_df.loc[(NEGA_DIFF< train_df['time_diff']) & (train_df['time_diff'] <= 0), 'x']\n",
    "# train_df.loc[(NEGA_DIFF < train_df['time_diff']) & (train_df['time_diff'] <= 0), 'wifi_y'] = train_df.loc[(NEGA_DIFF < train_df['time_diff']) & (train_df['time_diff'] <= 0), 'y']\n",
    "# train_df = train_df[(NEGA_DIFF < train_df['time_diff']) & (train_df['time_diff'] < POSI_DIFF)].reset_index(drop=True)\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BSSIDとRSSIは100ずつ存在しているけど全てが必要なわけではないみたい  \n",
    "ここでは20だけ取り出している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training target features\n",
    "NUM_FEATS = 80\n",
    "BSSID_FEATS = [f'bssid_{i}' for i in range(NUM_FEATS)]\n",
    "RSSI_FEATS  = [f'rssi_{i}' for i in range(NUM_FEATS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bssid_NはN個目のBSSIDを示しておりRSSI値が大きい順に番号が振られている。\n",
    "100個しかない\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get numbers of bssids to embed them in a layer\n",
    "\n",
    "# train\n",
    "wifi_bssids = []\n",
    "# bssidを列ごとにリストに入れていく\n",
    "for i in range(100):\n",
    "    wifi_bssids.extend(train_df.iloc[:,i].values.tolist())\n",
    "wifi_bssids = list(set(wifi_bssids))\n",
    "\n",
    "train_wifi_bssids_size = len(wifi_bssids)\n",
    "print(f'BSSID TYPES(train): {train_wifi_bssids_size}')\n",
    "\n",
    "# test\n",
    "wifi_bssids_test = []\n",
    "for i in range(100):\n",
    "    wifi_bssids_test.extend(test_df.iloc[:,i].values.tolist())\n",
    "wifi_bssids_test = list(set(wifi_bssids_test))\n",
    "\n",
    "test_wifi_bssids_size = len(wifi_bssids_test)\n",
    "print(f'BSSID TYPES(test): {test_wifi_bssids_size}')\n",
    "\n",
    "\n",
    "wifi_bssids.extend(wifi_bssids_test)\n",
    "wifi_bssids = list(set(wifi_bssids))\n",
    "wifi_bssids_size = len(wifi_bssids)\n",
    "print(f'BSSID TYPES(all): {wifi_bssids_size}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(wifi_bssids)\n",
    "le_site = LabelEncoder()\n",
    "le_site.fit(train_df['site_id'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(train_df.loc[:,RSSI_FEATS])\n",
    "\n",
    "\n",
    "def preprocess(input_df, le=le, le_site=le_site, ss=ss):\n",
    "    output_df = input_df.copy()\n",
    "    # RSSIの正規化\n",
    "    output_df.loc[:,RSSI_FEATS] = ss.transform(input_df.loc[:,RSSI_FEATS])\n",
    "\n",
    "    # BSSIDのLE(1からふる)\n",
    "    for i in BSSID_FEATS:\n",
    "        output_df.loc[:,i] = le.transform(input_df.loc[:,i])\n",
    "        output_df.loc[:,i] = output_df.loc[:,i] + 1  # 0からではなく1から番号を振りたいため なぜ？ embeddingのpadding用のダミー変数？\n",
    "\n",
    "    # site_idのLE\n",
    "    output_df.loc[:, 'site_id'] = le_site.transform(input_df.loc[:, 'site_id'])\n",
    "\n",
    "    # なぜ２重でやる？\n",
    "    # output_df.loc[:,RSSI_FEATS] = ss.transform(output_df.loc[:,RSSI_FEATS])\n",
    "    return output_df\n",
    "\n",
    "train = preprocess(train_df)\n",
    "test = preprocess(test_df)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_count = len(train['site_id'].unique())\n",
    "site_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch model\n",
    "- embedding layerが重要  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class IndoorDataset(Dataset):\n",
    "    def __init__(self, df, phase='train'):\n",
    "        self.df = df\n",
    "        self.phase = phase\n",
    "        self.bssid_feats = df[BSSID_FEATS].values.astype(int)\n",
    "        self.rssi_feats = df[RSSI_FEATS].values.astype(np.float32)\n",
    "        self.site_id = df['site_id'].values.astype(int)\n",
    "\n",
    "        if phase in ['train', 'valid']:\n",
    "            # self.xy = df[['x', 'y']].values.astype(np.float32)\n",
    "            self.xy = df[['wifi_x', 'wifi_y']].values.astype(np.float32)  # wifiにより補正したx,yを使用\n",
    "            self.floor = df['floor'].values.astype(np.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        feature = {\n",
    "            'BSSID_FEATS':self.bssid_feats[idx],\n",
    "            'RSSI_FEATS':self.rssi_feats[idx],\n",
    "            'site_id':self.site_id[idx]\n",
    "        }\n",
    "        if self.phase in ['train', 'valid']:\n",
    "            target = {\n",
    "                'xy':self.xy[idx],\n",
    "                'floor':self.floor[idx]\n",
    "            }\n",
    "        else:\n",
    "            target = {}\n",
    "        return feature, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, wifi_bssids_size, site_count=24, embedding_dim=64):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # bssid\n",
    "        # ->64次元に圧縮後sequence化にする\n",
    "        # wifi_bssids_sizeが辞書の数を表す\n",
    "        self.embedding_layer1 = nn.Sequential(\n",
    "            nn.Embedding(wifi_bssids_size, embedding_dim),\n",
    "            nn.Flatten(start_dim=-2)            \n",
    "        )\n",
    "        # site\n",
    "        # ->2次元に圧縮後sequence化する\n",
    "        # site_countが辞書の数を表す\n",
    "        self.embedding_layer2 = nn.Sequential(\n",
    "            nn.Embedding(site_count, 2),\n",
    "            nn.Flatten(start_dim=-1)           \n",
    "        )\n",
    "\n",
    "        # rssi\n",
    "        # 次元を64倍に線形変換\n",
    "        self.linear_layer1 = nn.Sequential(\n",
    "            nn.BatchNorm1d(NUM_FEATS),\n",
    "            nn.Linear(NUM_FEATS, NUM_FEATS * embedding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        # bssid, site, rssiの出力size\n",
    "        feature_size = 2 + (2 * NUM_FEATS * embedding_dim)\n",
    "        self.linear_layer2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(feature_size),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(feature_size, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(1)\n",
    "        self.lstm1 = nn.LSTM(input_size=256,hidden_size=128,dropout=0.3, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128,hidden_size=16,dropout=0.1, batch_first=True)\n",
    "\n",
    "        self.fc_xy = nn.Linear(16, 2)\n",
    "        # self.fc_x = nn.Linear(16, 1)\n",
    "        # self.fc_y = nn.Linear(16, 1)\n",
    "        self.fc_floor = nn.Linear(16, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input embedding\n",
    "        batch_size = x[\"site_id\"].shape[0]\n",
    "        x_bssid = self.embedding_layer1(x['BSSID_FEATS'])\n",
    "        x_site_id = self.embedding_layer2(x['site_id'])\n",
    "        x_rssi = self.linear_layer1(x['RSSI_FEATS'])\n",
    "        x = torch.cat([x_bssid, x_site_id, x_rssi], dim=-1)\n",
    "        x = self.linear_layer2(x)\n",
    "\n",
    "        # lstm layer\n",
    "        x = x.view(batch_size, 1, -1)  # [batch, 1]->[batch, 1, 1]\n",
    "        x = self.batch_norm1(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = torch.relu(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # output [batch, 1, 1] -> [batch]\n",
    "        # x_ = self.fc_x(x).view(-1)\n",
    "        # y_ = self.fc_y(x).view(-1)\n",
    "        xy = self.fc_xy(x).squeeze(1)\n",
    "        floor = torch.relu(self.fc_floor(x)).view(-1)\n",
    "        # return {\"x\":x_, \"y\":y_, \"floor\":floor} \n",
    "        return {\"xy\": xy, \"floor\": floor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_position_error(xhat, yhat, fhat, x, y, f):\n",
    "    intermediate = np.sqrt(np.power(xhat-x, 2) + np.power(yhat-y, 2)) + 15 * np.abs(fhat-f)\n",
    "    return intermediate.sum()/xhat.shape[0]\n",
    "\n",
    "def to_np(input):\n",
    "    return input.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model: nn.Module, config: dict):\n",
    "    optimizer_config = config[\"optimizer\"]\n",
    "    optimizer_name = optimizer_config.get(\"name\")\n",
    "    base_optimizer_name = optimizer_config.get(\"base_name\")\n",
    "    optimizer_params = optimizer_config['params']\n",
    "\n",
    "    if hasattr(optim, optimizer_name):\n",
    "        optimizer = optim.__getattribute__(optimizer_name)(model.parameters(), **optimizer_params)\n",
    "        return optimizer\n",
    "    else:\n",
    "        base_optimizer = optim.__getattribute__(base_optimizer_name)\n",
    "        optimizer = globals().get(optimizer_name)(\n",
    "            model.parameters(), \n",
    "            base_optimizer,\n",
    "            **optimizer_config[\"params\"])\n",
    "        return  optimizer\n",
    "\n",
    "def get_scheduler(optimizer, config: dict):\n",
    "    scheduler_config = config[\"scheduler\"]\n",
    "    scheduler_name = scheduler_config.get(\"name\")\n",
    "\n",
    "    if scheduler_name is None:\n",
    "        return\n",
    "    else:\n",
    "        return optim.lr_scheduler.__getattribute__(scheduler_name)(\n",
    "            optimizer, **scheduler_config[\"params\"])\n",
    "\n",
    "\n",
    "def get_criterion(config: dict):\n",
    "    loss_config = config[\"loss\"]\n",
    "    loss_name = loss_config[\"name\"]\n",
    "    loss_params = {} if loss_config.get(\"params\") is None else loss_config.get(\"params\")\n",
    "    if hasattr(nn, loss_name):\n",
    "        criterion = nn.__getattribute__(loss_name)(**loss_params)\n",
    "    else:\n",
    "        criterion = globals().get(loss_name)(**loss_params)\n",
    "\n",
    "    return criterion\n",
    "\n",
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner class(pytorch-lighting)\n",
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.xy_criterion = get_criterion(config)\n",
    "        self.f_criterion = get_criterion(config)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self.model(x)\n",
    "        loss = self.xy_criterion(output[\"xy\"], y[\"xy\"])\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self.model(x)\n",
    "        xy_loss = self.xy_criterion(output[\"xy\"], y[\"xy\"])\n",
    "        f_loss = self.f_criterion(output[\"floor\"], y[\"floor\"])\n",
    "        loss = xy_loss  # + f_loss\n",
    "        mpe = mean_position_error(\n",
    "            to_np(output['xy'][:, 0]), to_np(output['xy'][:, 1]), 0, \n",
    "            to_np(y['xy'][:, 0]), to_np(y['xy'][:, 1]), 0)\n",
    "        \n",
    "        # floor lossは現状は無視して良い\n",
    "        self.log(f'Loss/val', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'Loss/xy', xy_loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'Loss/floor', f_loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'MPE/val', mpe, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = get_optimizer(self.model, self.config)\n",
    "        scheduler = get_scheduler(optimizer, self.config)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"Loss/val\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof\n",
    "def evaluate(model, loaders, phase):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    f_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loaders[phase]:\n",
    "            x, y = batch\n",
    "            output = model(x)\n",
    "            x_list.append(to_np(output['xy'][:, 0]))\n",
    "            y_list.append(to_np(output['xy'][:, 1]))\n",
    "            f_list.append(to_np(output['floor']))\n",
    "\n",
    "    x_list = np.concatenate(x_list)\n",
    "    y_list = np.concatenate(y_list)\n",
    "    f_list = np.concatenate(f_list)\n",
    "    return x_list, y_list, f_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oofs = []  # 全てのoofをdfで格納する\n",
    "predictions = []  # 全ての予測値をdfで格納する\n",
    "val_scores = []\n",
    "# skf = model_selection.StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "gkf = model_selection.GroupKFold(n_splits=N_SPLITS)\n",
    "# 今回はtargetを均等に分ける必要はなくpathが均等に分かれればいいのでskf.split()にpathを与えている。\n",
    "for fold, (trn_idx, val_idx) in enumerate(gkf.split(train.loc[:, 'path'], groups=train.loc[:, 'path'])):\n",
    "    # 指定したfoldのみループを回す\n",
    "    if fold not in USE_FOLDS:\n",
    "        continue\n",
    "\n",
    "    print('=' * 20)\n",
    "    print(f'Fold {fold}')\n",
    "    print('=' * 20)\n",
    "\n",
    "    # train/valid data\n",
    "    trn_df = train.loc[trn_idx, BSSID_FEATS + RSSI_FEATS + ['site_id', 'wifi_x','wifi_y','floor']].reset_index(drop=True)\n",
    "    val_df = train.loc[val_idx, BSSID_FEATS + RSSI_FEATS + ['site_id', 'wifi_x','wifi_y','floor']].reset_index(drop=True)\n",
    "\n",
    "    # data loader\n",
    "    loaders = {}\n",
    "    loader_config = config[\"loader\"]\n",
    "    loaders[\"train\"] = DataLoader(IndoorDataset(trn_df, phase=\"train\"), **loader_config[\"train\"], worker_init_fn=worker_init_fn) \n",
    "    loaders[\"valid\"] = DataLoader(IndoorDataset(val_df, phase=\"valid\"), **loader_config[\"valid\"], worker_init_fn=worker_init_fn)\n",
    "    loaders[\"test\"] = DataLoader(IndoorDataset(test, phase=\"test\"), **loader_config[\"test\"], worker_init_fn=worker_init_fn)\n",
    "\n",
    "    # model\n",
    "    model = LSTMModel(wifi_bssids_size+1, site_count)  # +1としているのはLEを1スタートで始めているため\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    # callbacks\n",
    "    callbacks = []\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=f'Loss/val',\n",
    "        mode='min',\n",
    "        dirpath=f\"../../model/{EXP_NAME}\",\n",
    "        verbose=False,\n",
    "        filename=f'{model_name}-{fold}')\n",
    "    \n",
    "    if MODEL_SAVE:\n",
    "        callbacks.append(checkpoint_callback)\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='Loss/val',\n",
    "        min_delta=0.00,\n",
    "        patience=10,\n",
    "        verbose=False,\n",
    "        mode='min')\n",
    "    callbacks.append(early_stop_callback)\n",
    "\n",
    "    # loggers\n",
    "    RUN_NAME = EXP_NAME + \"_\" + EXP_MESSAGE\n",
    "    wandb.init(project='indoor', notes=NOTES, entity='kuto5046', group=RUN_NAME)\n",
    "    wandb.run.name = RUN_NAME + f'-fold-{fold}'\n",
    "    wandb_config = wandb.config\n",
    "    wandb_config.model_name = model_name\n",
    "    wandb_config.LB = None\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    \n",
    "    loggers = []\n",
    "    loggers.append(WandbLogger())\n",
    "\n",
    "    learner = Learner(model, config)\n",
    "    # pretrained flag\n",
    "    if PRETRAINED:\n",
    "        ckpt = torch.load(PRETRAINED_PATH + f'{model_name}-{fold}.ckpt')\n",
    "        learner.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        logger=loggers, \n",
    "        callbacks=callbacks,\n",
    "        max_epochs=MAX_EPOCHS,\n",
    "        gpus=[0],\n",
    "        fast_dev_run=DEBUG,\n",
    "        deterministic=True,\n",
    "        # precision=16,\n",
    "        progress_bar_refresh_rate=0  # vscodeの時progress barの動作が遅いので表示しない\n",
    "        )\n",
    "\n",
    "    trainer.fit(learner, train_dataloader=loaders['train'], val_dataloaders=loaders['valid'])\n",
    "\n",
    "    #############\n",
    "    # validation (to make oof)\n",
    "    #############\n",
    "    model.eval()\n",
    "    oof_df = train.loc[val_idx, ['timestamp', 'x', 'y', 'site_id', 'wifi_x','wifi_y', 'floor', 'floor_str', 'path', 'time_diff', 'timestamp']].reset_index(drop=True)\n",
    "    oof_x, oof_y, oof_f = evaluate(model, loaders, phase=\"valid\")\n",
    "    val_df[\"oof_x\"] = oof_x\n",
    "    val_df[\"oof_y\"] = oof_y\n",
    "    val_df[\"oof_floor\"] = oof_f\n",
    "    oofs.append(val_df)\n",
    "    \n",
    "    val_score = mean_position_error(\n",
    "        val_df[\"oof_x\"].values, val_df[\"oof_y\"].values, 0,\n",
    "        val_df['wifi_x'].values, val_df['wifi_y'].values, 0)\n",
    "    val_scores.append(val_score)\n",
    "    print(f\"fold {fold}: mean position error {val_score}\")\n",
    "\n",
    "    #############\n",
    "    # inference\n",
    "    #############n\n",
    "\n",
    "    preds_x, preds_y, preds_f = evaluate(model, loaders, phase=\"test\")\n",
    "    test_preds = pd.DataFrame(np.stack((preds_f, preds_x, preds_y))).T\n",
    "    test_preds.columns = sub.columns\n",
    "    test_preds[\"site_path_timestamp\"] = test[\"site_path_timestamp\"]\n",
    "    test_preds[\"floor\"] = test_preds[\"floor\"].astype(int)\n",
    "    test_preds.to_csv(f'{EXP_NAME}_fold{fold}.csv', index=False)\n",
    "    predictions.append(test_preds)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(USE_FOLDS) > 1:\n",
    "    oofs_df = pd.concat(oofs)\n",
    "else:\n",
    "    oofs_df = oofs[0]\n",
    "oofs_df.to_csv(\"oof.csv\", index=False)\n",
    "oofs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(USE_FOLDS) > 1:\n",
    "    # foldの結果を平均した後、reindexでsubmission fileにindexを合わせる\n",
    "    all_preds = pd.concat(predictions).groupby('site_path_timestamp').mean().reindex(sub.index)\n",
    "else:\n",
    "    all_preds = predictions[0].reindex(sub.index)\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floorの数値を置換\n",
    "simple_accurate_99 = pd.read_csv(root_dir / 'simple-99-accurate-floor-model/submission.csv')\n",
    "all_preds['floor'] = simple_accurate_99['floor'].values\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds.to_csv(RUN_NAME + '.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"CV:{np.mean(val_scores)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}