{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import yaml\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.utilities.seed import seed_everything"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Global seed set to 1996\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1996"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "# config\n",
    "with open('config.yaml') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# globals variable\n",
    "SEED = config['globals']['seed']\n",
    "MAX_EPOCHS = config['globals']['max_epochs']\n",
    "N_SPLITS = config['globals']['n_splits']\n",
    "USE_FOLDS = config['globals']['use_folds']\n",
    "DEBUG = config['globals']['debug']\n",
    "EXP_MESSAGE = config['globals']['exp_message']\n",
    "NOTES = config['globals']['notes']\n",
    "MODEL_SAVE = config['globals']['model_save']\n",
    "ONLY_PRED = config['globals']['only_pred']\n",
    "PRETRAINED = config['globals']['pretrained']\n",
    "PRETRAINED_PATH = config['globals']['pretrained_path']\n",
    "EXP_NAME = str(Path().resolve()).split('/')[-1]\n",
    "\n",
    "# seed\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'exp132'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "EXP_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/user/.netrc\n"
     ]
    }
   ],
   "source": [
    "!wandb login 1bb2d0449c11d8b987e25c38b9d8dda176310fb6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waypointを補正したdataset\n",
    "root_dir = Path('../../input/')\n",
    "with open(root_dir/'2kaido_wifi_dataset_v5/train_10000_7.pkl', 'rb') as f:\n",
    "  train_df = pickle.load(f)\n",
    "\n",
    "with open(root_dir/'2kaido_wifi_dataset_v5/test_10000_7.pkl', 'rb') as f:\n",
    "  test_df = pickle.load(f)\n",
    "\n",
    "sub_df = pd.read_csv(root_dir/'indoor-location-navigation/sample_submission.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.astype({'timestamp':int,'x': np.float32, 'y': np.float32, 'floor':np.float32, 'ix':np.float32, 'iy':np.float32,  'fx':np.float32, 'fy':np.float32})\n",
    "train_df['floor'] = train_df['floor'].astype(int)  # str -> float -> intで負の数をintにする\n",
    "test_df = test_df.astype({'timestamp':int})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test用のfloor特徴量\n",
    "floor_sub = pd.read_csv(root_dir / 'base_lb3.727_BiLSTM_skf_cv999.csv')  # しんちろさんのsub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_df.merge(floor_sub[['site_path_timestamp','floor']], on='site_path_timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "DISTANCE_THR = 1\n",
    "train_df['distance'] = np.sqrt((train_df['ix']-train_df['fx'])**2 + (train_df['iy']-train_df['fy'])**2)\n",
    "itrain_df = train_df.drop(['fx','fy'], axis=1).copy()  # linearのtrain\n",
    "ftrain_df = train_df.drop(['ix','iy'], axis=1).copy()  # kalmanのtrain \n",
    "ftrain_df = ftrain_df[ftrain_df['distance']<DISTANCE_THR].reset_index(drop=True)   # 5<distanceは信頼性低いので削る\n",
    "ftrain_df = ftrain_df.rename(columns={'fx':'ix', 'fy':'iy'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([itrain_df, ftrain_df]).drop_duplicates().reset_index(drop=True)  # 重複削除によって 503421 -> 503119"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            id                                    ssid_0  \\\n",
       "0            0  356d66c73c423be835bd5d07cb1ebdfa821d8e23   \n",
       "1            1  356d66c73c423be835bd5d07cb1ebdfa821d8e23   \n",
       "2            2  f44fa6118fed7198296c8b45b2f2684903d99620   \n",
       "3            3  356d66c73c423be835bd5d07cb1ebdfa821d8e23   \n",
       "4            4  f44fa6118fed7198296c8b45b2f2684903d99620   \n",
       "...        ...                                       ...   \n",
       "449701  449701  072159287058774aa9b450c9163129b309bc982b   \n",
       "449702  449702  18336f0964fbd470a2ee7116d33b96946c633999   \n",
       "449703  449703  18336f0964fbd470a2ee7116d33b96946c633999   \n",
       "449704  449704  18336f0964fbd470a2ee7116d33b96946c633999   \n",
       "449705  449705  18336f0964fbd470a2ee7116d33b96946c633999   \n",
       "\n",
       "                                          ssid_1  \\\n",
       "0       c3513a636d1a813db081a2ffc33f297b4fefe28d   \n",
       "1       f44fa6118fed7198296c8b45b2f2684903d99620   \n",
       "2       356d66c73c423be835bd5d07cb1ebdfa821d8e23   \n",
       "3       f44fa6118fed7198296c8b45b2f2684903d99620   \n",
       "4       356d66c73c423be835bd5d07cb1ebdfa821d8e23   \n",
       "...                                          ...   \n",
       "449701  072159287058774aa9b450c9163129b309bc982b   \n",
       "449702  072159287058774aa9b450c9163129b309bc982b   \n",
       "449703  18336f0964fbd470a2ee7116d33b96946c633999   \n",
       "449704  18336f0964fbd470a2ee7116d33b96946c633999   \n",
       "449705  90ef6526fa145d95dc065edc3b3a3193406f3ee4   \n",
       "\n",
       "                                          ssid_2  \\\n",
       "0       990847ae755de95a44c7b1be54e66f4e56fdc7f4   \n",
       "1       c3513a636d1a813db081a2ffc33f297b4fefe28d   \n",
       "2       8aed75f7c344e6a3d4916750029dd4ee47c1e7c5   \n",
       "3       5a368e0bd5050bdb4653dc39c86bee1fd2b8aeb7   \n",
       "4       990847ae755de95a44c7b1be54e66f4e56fdc7f4   \n",
       "...                                          ...   \n",
       "449701  18336f0964fbd470a2ee7116d33b96946c633999   \n",
       "449702  18336f0964fbd470a2ee7116d33b96946c633999   \n",
       "449703  072159287058774aa9b450c9163129b309bc982b   \n",
       "449704  072159287058774aa9b450c9163129b309bc982b   \n",
       "449705  040667e2fdd3a9bbb54e970c430ad6f08416c52e   \n",
       "\n",
       "                                          ssid_3  \\\n",
       "0       4e601619b7c7d9df8d61490ad2c134f08ea01d61   \n",
       "1       990847ae755de95a44c7b1be54e66f4e56fdc7f4   \n",
       "2       c3513a636d1a813db081a2ffc33f297b4fefe28d   \n",
       "3       990847ae755de95a44c7b1be54e66f4e56fdc7f4   \n",
       "4       c3513a636d1a813db081a2ffc33f297b4fefe28d   \n",
       "...                                          ...   \n",
       "449701  da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "449702  040667e2fdd3a9bbb54e970c430ad6f08416c52e   \n",
       "449703  072159287058774aa9b450c9163129b309bc982b   \n",
       "449704  040667e2fdd3a9bbb54e970c430ad6f08416c52e   \n",
       "449705  18336f0964fbd470a2ee7116d33b96946c633999   \n",
       "\n",
       "                                          ssid_4  \\\n",
       "0       c3513a636d1a813db081a2ffc33f297b4fefe28d   \n",
       "1       d839a45ebe64ab48b60a407d837fb01d3c0dfef9   \n",
       "2       5a368e0bd5050bdb4653dc39c86bee1fd2b8aeb7   \n",
       "3       da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "4       d839a45ebe64ab48b60a407d837fb01d3c0dfef9   \n",
       "...                                          ...   \n",
       "449701  18336f0964fbd470a2ee7116d33b96946c633999   \n",
       "449702  072159287058774aa9b450c9163129b309bc982b   \n",
       "449703  ea9e102c49dbc834b1059c5e8b5dc4a017a82dff   \n",
       "449704  da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "449705  da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "\n",
       "                                          ssid_5  \\\n",
       "0       b14c56d7cd73a5b026fc118c671e24dc2ba2558b   \n",
       "1       b7e6027447eb1f81327d66cfd3adbe557aabf26c   \n",
       "2       4e601619b7c7d9df8d61490ad2c134f08ea01d61   \n",
       "3       b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7   \n",
       "4       b7e6027447eb1f81327d66cfd3adbe557aabf26c   \n",
       "...                                          ...   \n",
       "449701  da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "449702  da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "449703  040667e2fdd3a9bbb54e970c430ad6f08416c52e   \n",
       "449704  072159287058774aa9b450c9163129b309bc982b   \n",
       "449705  072159287058774aa9b450c9163129b309bc982b   \n",
       "\n",
       "                                          ssid_6  \\\n",
       "0       d839a45ebe64ab48b60a407d837fb01d3c0dfef9   \n",
       "1       7182afc4e5c212133d5d7d76eb3df6c24618302b   \n",
       "2       2ce029b3a3dea973a44bf0587be5e2b93a74e5f1   \n",
       "3       b7e6027447eb1f81327d66cfd3adbe557aabf26c   \n",
       "4       7182afc4e5c212133d5d7d76eb3df6c24618302b   \n",
       "...                                          ...   \n",
       "449701  da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "449702  da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "449703  da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "449704  90ef6526fa145d95dc065edc3b3a3193406f3ee4   \n",
       "449705  ea9e102c49dbc834b1059c5e8b5dc4a017a82dff   \n",
       "\n",
       "                                          ssid_7  \\\n",
       "0       7182afc4e5c212133d5d7d76eb3df6c24618302b   \n",
       "1       da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "2       b14c56d7cd73a5b026fc118c671e24dc2ba2558b   \n",
       "3       3fa90121039c7b6e24ae985d228e0366ae15fba4   \n",
       "4       b9f0208be00bd8b337be7f12e02e3a3ce846e22b   \n",
       "...                                          ...   \n",
       "449701  90ef6526fa145d95dc065edc3b3a3193406f3ee4   \n",
       "449702  da39a3ee5e6b4b0d3255bfef95601890afd80709   \n",
       "449703  ea9e102c49dbc834b1059c5e8b5dc4a017a82dff   \n",
       "449704  ea9e102c49dbc834b1059c5e8b5dc4a017a82dff   \n",
       "449705  072159287058774aa9b450c9163129b309bc982b   \n",
       "\n",
       "                                          ssid_8  ...     itimestamp  \\\n",
       "0       b9f0208be00bd8b337be7f12e02e3a3ce846e22b  ...  1578466134278   \n",
       "1       b9f0208be00bd8b337be7f12e02e3a3ce846e22b  ...  1578466136278   \n",
       "2       3fa90121039c7b6e24ae985d228e0366ae15fba4  ...  1578466138278   \n",
       "3       d839a45ebe64ab48b60a407d837fb01d3c0dfef9  ...  1578466140278   \n",
       "4       b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7  ...  1578466142278   \n",
       "...                                          ...  ...            ...   \n",
       "449701  da39a3ee5e6b4b0d3255bfef95601890afd80709  ...  1573892864000   \n",
       "449702  da39a3ee5e6b4b0d3255bfef95601890afd80709  ...  1573892866000   \n",
       "449703  da39a3ee5e6b4b0d3255bfef95601890afd80709  ...  1573892868000   \n",
       "449704  1f09251bbfadafb11c63c87963af25238d6bc886  ...  1573892871500   \n",
       "449705  1f09251bbfadafb11c63c87963af25238d6bc886  ...  1573892873500   \n",
       "\n",
       "                ix          iy floor floor_str                      path  \\\n",
       "0       113.615387  156.634796    -1        B1  5e1580d1f4c3420006d520e4   \n",
       "1       112.655884  156.358200    -1        B1  5e1580d1f4c3420006d520e4   \n",
       "2       111.936256  156.150772    -1        B1  5e1580d1f4c3420006d520e4   \n",
       "3       110.976761  155.874176    -1        B1  5e1580d1f4c3420006d520e4   \n",
       "4       110.017258  155.597580    -1        B1  5e1580d1f4c3420006d520e4   \n",
       "...            ...         ...   ...       ...                       ...   \n",
       "449701  126.694611  107.063019     6        F7  5dcfb393878f3300066c70a6   \n",
       "449702  128.817688  108.838440     6        F7  5dcfb393878f3300066c70a6   \n",
       "449703  131.251892  111.029167     6        F7  5dcfb393878f3300066c70a6   \n",
       "449704  134.495392  114.157982     6        F7  5dcfb393878f3300066c70a6   \n",
       "449705  136.471344  116.230675     6        F7  5dcfb393878f3300066c70a6   \n",
       "\n",
       "                         site_id timediff itimediff  distance  \n",
       "0       5a0546857ecc773753327266    -1545       -45  0.587131  \n",
       "1       5a0546857ecc773753327266    -3512       -12  0.772321  \n",
       "2       5a0546857ecc773753327266    -5471        29  1.045990  \n",
       "3       5a0546857ecc773753327266    -7428        72  1.889729  \n",
       "4       5a0546857ecc773753327266     7401       105  2.400107  \n",
       "...                          ...      ...       ...       ...  \n",
       "449701  5dc8cea7659e181adb076a3f     -291       -29  0.081698  \n",
       "449702  5dc8cea7659e181adb076a3f    -2224        38  0.209345  \n",
       "449703  5dc8cea7659e181adb076a3f     1098       126  0.789737  \n",
       "449704  5dc8cea7659e181adb076a3f     2619      -187  0.713653  \n",
       "449705  5dc8cea7659e181adb076a3f      730       -76  0.513447  \n",
       "\n",
       "[449706 rows x 514 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>ssid_0</th>\n      <th>ssid_1</th>\n      <th>ssid_2</th>\n      <th>ssid_3</th>\n      <th>ssid_4</th>\n      <th>ssid_5</th>\n      <th>ssid_6</th>\n      <th>ssid_7</th>\n      <th>ssid_8</th>\n      <th>...</th>\n      <th>itimestamp</th>\n      <th>ix</th>\n      <th>iy</th>\n      <th>floor</th>\n      <th>floor_str</th>\n      <th>path</th>\n      <th>site_id</th>\n      <th>timediff</th>\n      <th>itimediff</th>\n      <th>distance</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>356d66c73c423be835bd5d07cb1ebdfa821d8e23</td>\n      <td>c3513a636d1a813db081a2ffc33f297b4fefe28d</td>\n      <td>990847ae755de95a44c7b1be54e66f4e56fdc7f4</td>\n      <td>4e601619b7c7d9df8d61490ad2c134f08ea01d61</td>\n      <td>c3513a636d1a813db081a2ffc33f297b4fefe28d</td>\n      <td>b14c56d7cd73a5b026fc118c671e24dc2ba2558b</td>\n      <td>d839a45ebe64ab48b60a407d837fb01d3c0dfef9</td>\n      <td>7182afc4e5c212133d5d7d76eb3df6c24618302b</td>\n      <td>b9f0208be00bd8b337be7f12e02e3a3ce846e22b</td>\n      <td>...</td>\n      <td>1578466134278</td>\n      <td>113.615387</td>\n      <td>156.634796</td>\n      <td>-1</td>\n      <td>B1</td>\n      <td>5e1580d1f4c3420006d520e4</td>\n      <td>5a0546857ecc773753327266</td>\n      <td>-1545</td>\n      <td>-45</td>\n      <td>0.587131</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>356d66c73c423be835bd5d07cb1ebdfa821d8e23</td>\n      <td>f44fa6118fed7198296c8b45b2f2684903d99620</td>\n      <td>c3513a636d1a813db081a2ffc33f297b4fefe28d</td>\n      <td>990847ae755de95a44c7b1be54e66f4e56fdc7f4</td>\n      <td>d839a45ebe64ab48b60a407d837fb01d3c0dfef9</td>\n      <td>b7e6027447eb1f81327d66cfd3adbe557aabf26c</td>\n      <td>7182afc4e5c212133d5d7d76eb3df6c24618302b</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>b9f0208be00bd8b337be7f12e02e3a3ce846e22b</td>\n      <td>...</td>\n      <td>1578466136278</td>\n      <td>112.655884</td>\n      <td>156.358200</td>\n      <td>-1</td>\n      <td>B1</td>\n      <td>5e1580d1f4c3420006d520e4</td>\n      <td>5a0546857ecc773753327266</td>\n      <td>-3512</td>\n      <td>-12</td>\n      <td>0.772321</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>f44fa6118fed7198296c8b45b2f2684903d99620</td>\n      <td>356d66c73c423be835bd5d07cb1ebdfa821d8e23</td>\n      <td>8aed75f7c344e6a3d4916750029dd4ee47c1e7c5</td>\n      <td>c3513a636d1a813db081a2ffc33f297b4fefe28d</td>\n      <td>5a368e0bd5050bdb4653dc39c86bee1fd2b8aeb7</td>\n      <td>4e601619b7c7d9df8d61490ad2c134f08ea01d61</td>\n      <td>2ce029b3a3dea973a44bf0587be5e2b93a74e5f1</td>\n      <td>b14c56d7cd73a5b026fc118c671e24dc2ba2558b</td>\n      <td>3fa90121039c7b6e24ae985d228e0366ae15fba4</td>\n      <td>...</td>\n      <td>1578466138278</td>\n      <td>111.936256</td>\n      <td>156.150772</td>\n      <td>-1</td>\n      <td>B1</td>\n      <td>5e1580d1f4c3420006d520e4</td>\n      <td>5a0546857ecc773753327266</td>\n      <td>-5471</td>\n      <td>29</td>\n      <td>1.045990</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>356d66c73c423be835bd5d07cb1ebdfa821d8e23</td>\n      <td>f44fa6118fed7198296c8b45b2f2684903d99620</td>\n      <td>5a368e0bd5050bdb4653dc39c86bee1fd2b8aeb7</td>\n      <td>990847ae755de95a44c7b1be54e66f4e56fdc7f4</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7</td>\n      <td>b7e6027447eb1f81327d66cfd3adbe557aabf26c</td>\n      <td>3fa90121039c7b6e24ae985d228e0366ae15fba4</td>\n      <td>d839a45ebe64ab48b60a407d837fb01d3c0dfef9</td>\n      <td>...</td>\n      <td>1578466140278</td>\n      <td>110.976761</td>\n      <td>155.874176</td>\n      <td>-1</td>\n      <td>B1</td>\n      <td>5e1580d1f4c3420006d520e4</td>\n      <td>5a0546857ecc773753327266</td>\n      <td>-7428</td>\n      <td>72</td>\n      <td>1.889729</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>f44fa6118fed7198296c8b45b2f2684903d99620</td>\n      <td>356d66c73c423be835bd5d07cb1ebdfa821d8e23</td>\n      <td>990847ae755de95a44c7b1be54e66f4e56fdc7f4</td>\n      <td>c3513a636d1a813db081a2ffc33f297b4fefe28d</td>\n      <td>d839a45ebe64ab48b60a407d837fb01d3c0dfef9</td>\n      <td>b7e6027447eb1f81327d66cfd3adbe557aabf26c</td>\n      <td>7182afc4e5c212133d5d7d76eb3df6c24618302b</td>\n      <td>b9f0208be00bd8b337be7f12e02e3a3ce846e22b</td>\n      <td>b6ffe5619e02871fcd04f61c9bb4b5c53a3f46b7</td>\n      <td>...</td>\n      <td>1578466142278</td>\n      <td>110.017258</td>\n      <td>155.597580</td>\n      <td>-1</td>\n      <td>B1</td>\n      <td>5e1580d1f4c3420006d520e4</td>\n      <td>5a0546857ecc773753327266</td>\n      <td>7401</td>\n      <td>105</td>\n      <td>2.400107</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>449701</th>\n      <td>449701</td>\n      <td>072159287058774aa9b450c9163129b309bc982b</td>\n      <td>072159287058774aa9b450c9163129b309bc982b</td>\n      <td>18336f0964fbd470a2ee7116d33b96946c633999</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>18336f0964fbd470a2ee7116d33b96946c633999</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>90ef6526fa145d95dc065edc3b3a3193406f3ee4</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>...</td>\n      <td>1573892864000</td>\n      <td>126.694611</td>\n      <td>107.063019</td>\n      <td>6</td>\n      <td>F7</td>\n      <td>5dcfb393878f3300066c70a6</td>\n      <td>5dc8cea7659e181adb076a3f</td>\n      <td>-291</td>\n      <td>-29</td>\n      <td>0.081698</td>\n    </tr>\n    <tr>\n      <th>449702</th>\n      <td>449702</td>\n      <td>18336f0964fbd470a2ee7116d33b96946c633999</td>\n      <td>072159287058774aa9b450c9163129b309bc982b</td>\n      <td>18336f0964fbd470a2ee7116d33b96946c633999</td>\n      <td>040667e2fdd3a9bbb54e970c430ad6f08416c52e</td>\n      <td>072159287058774aa9b450c9163129b309bc982b</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>...</td>\n      <td>1573892866000</td>\n      <td>128.817688</td>\n      <td>108.838440</td>\n      <td>6</td>\n      <td>F7</td>\n      <td>5dcfb393878f3300066c70a6</td>\n      <td>5dc8cea7659e181adb076a3f</td>\n      <td>-2224</td>\n      <td>38</td>\n      <td>0.209345</td>\n    </tr>\n    <tr>\n      <th>449703</th>\n      <td>449703</td>\n      <td>18336f0964fbd470a2ee7116d33b96946c633999</td>\n      <td>18336f0964fbd470a2ee7116d33b96946c633999</td>\n      <td>072159287058774aa9b450c9163129b309bc982b</td>\n      <td>072159287058774aa9b450c9163129b309bc982b</td>\n      <td>ea9e102c49dbc834b1059c5e8b5dc4a017a82dff</td>\n      <td>040667e2fdd3a9bbb54e970c430ad6f08416c52e</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>ea9e102c49dbc834b1059c5e8b5dc4a017a82dff</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>...</td>\n      <td>1573892868000</td>\n      <td>131.251892</td>\n      <td>111.029167</td>\n      <td>6</td>\n      <td>F7</td>\n      <td>5dcfb393878f3300066c70a6</td>\n      <td>5dc8cea7659e181adb076a3f</td>\n      <td>1098</td>\n      <td>126</td>\n      <td>0.789737</td>\n    </tr>\n    <tr>\n      <th>449704</th>\n      <td>449704</td>\n      <td>18336f0964fbd470a2ee7116d33b96946c633999</td>\n      <td>18336f0964fbd470a2ee7116d33b96946c633999</td>\n      <td>072159287058774aa9b450c9163129b309bc982b</td>\n      <td>040667e2fdd3a9bbb54e970c430ad6f08416c52e</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>072159287058774aa9b450c9163129b309bc982b</td>\n      <td>90ef6526fa145d95dc065edc3b3a3193406f3ee4</td>\n      <td>ea9e102c49dbc834b1059c5e8b5dc4a017a82dff</td>\n      <td>1f09251bbfadafb11c63c87963af25238d6bc886</td>\n      <td>...</td>\n      <td>1573892871500</td>\n      <td>134.495392</td>\n      <td>114.157982</td>\n      <td>6</td>\n      <td>F7</td>\n      <td>5dcfb393878f3300066c70a6</td>\n      <td>5dc8cea7659e181adb076a3f</td>\n      <td>2619</td>\n      <td>-187</td>\n      <td>0.713653</td>\n    </tr>\n    <tr>\n      <th>449705</th>\n      <td>449705</td>\n      <td>18336f0964fbd470a2ee7116d33b96946c633999</td>\n      <td>90ef6526fa145d95dc065edc3b3a3193406f3ee4</td>\n      <td>040667e2fdd3a9bbb54e970c430ad6f08416c52e</td>\n      <td>18336f0964fbd470a2ee7116d33b96946c633999</td>\n      <td>da39a3ee5e6b4b0d3255bfef95601890afd80709</td>\n      <td>072159287058774aa9b450c9163129b309bc982b</td>\n      <td>ea9e102c49dbc834b1059c5e8b5dc4a017a82dff</td>\n      <td>072159287058774aa9b450c9163129b309bc982b</td>\n      <td>1f09251bbfadafb11c63c87963af25238d6bc886</td>\n      <td>...</td>\n      <td>1573892873500</td>\n      <td>136.471344</td>\n      <td>116.230675</td>\n      <td>6</td>\n      <td>F7</td>\n      <td>5dcfb393878f3300066c70a6</td>\n      <td>5dc8cea7659e181adb076a3f</td>\n      <td>730</td>\n      <td>-76</td>\n      <td>0.513447</td>\n    </tr>\n  </tbody>\n</table>\n<p>449706 rows × 514 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "train_df = train_df.reset_index().rename(columns={'index':'id'})\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove_id = np.load('40s_over_id.npy', allow_pickle=True)\n",
    "# remove_id.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df = train_df[~train_df['id'].isin(remove_id)].reset_index(drop=True)\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training target features\n",
    "NUM_FEATS = 80\n",
    "BSSID_FEATS = [f'bssid_{i}' for i in range(NUM_FEATS)]\n",
    "RSSI_FEATS  = [f'rssi_{i}' for i in range(NUM_FEATS)]\n",
    "TIMEDIFF_FEATS  = [f'timediff_{i}' for i in range(NUM_FEATS)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bssid_NはN個目のBSSIDを示しておりRSSI値が大きい順に番号が振られている。\n",
    "100個しかない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "BSSID TYPES(train): 60556\nBSSID TYPES(test): 30911\nBSSID TYPES(all): 60661\n"
     ]
    }
   ],
   "source": [
    "# get numbers of bssids to embed them in a layer\n",
    "\n",
    "# train\n",
    "wifi_bssids = []\n",
    "# bssidを列ごとにリストに入れていく\n",
    "for i in BSSID_FEATS:\n",
    "    wifi_bssids.extend(train_df.loc[:,i].values.tolist())\n",
    "wifi_bssids = list(set(wifi_bssids))\n",
    "\n",
    "train_wifi_bssids_size = len(wifi_bssids)\n",
    "print(f'BSSID TYPES(train): {train_wifi_bssids_size}')\n",
    "\n",
    "# test\n",
    "wifi_bssids_test = []\n",
    "for i in BSSID_FEATS:\n",
    "    wifi_bssids_test.extend(test_df.loc[:,i].values.tolist())\n",
    "wifi_bssids_test = list(set(wifi_bssids_test))\n",
    "\n",
    "test_wifi_bssids_size = len(wifi_bssids_test)\n",
    "print(f'BSSID TYPES(test): {test_wifi_bssids_size}')\n",
    "\n",
    "\n",
    "wifi_bssids.extend(wifi_bssids_test)\n",
    "wifi_bssids = list(set(wifi_bssids))\n",
    "wifi_bssids_size = len(wifi_bssids)\n",
    "print(f'BSSID TYPES(all): {wifi_bssids_size}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "RSSI TYPES(train): 97\nRSSI TYPES(test): 82\nRSSI TYPES(all): 98\n"
     ]
    }
   ],
   "source": [
    "# get numbers of bssids to embed them in a layer\n",
    "\n",
    "# train\n",
    "rssi_bssids = []\n",
    "# bssidを列ごとにリストに入れていく\n",
    "for i in RSSI_FEATS:\n",
    "    rssi_bssids.extend(train_df.loc[:,i].values.tolist())\n",
    "rssi_bssids = list(set(rssi_bssids))\n",
    "\n",
    "train_rssi_bssids_size = len(rssi_bssids)\n",
    "print(f'RSSI TYPES(train): {train_rssi_bssids_size}')\n",
    "\n",
    "# test\n",
    "rssi_bssids_test = []\n",
    "for i in RSSI_FEATS:\n",
    "    rssi_bssids_test.extend(test_df.loc[:,i].values.tolist())\n",
    "rssi_bssids_test = list(set(rssi_bssids_test))\n",
    "\n",
    "test_rssi_bssids_size = len(rssi_bssids_test)\n",
    "print(f'RSSI TYPES(test): {test_rssi_bssids_size}')\n",
    "\n",
    "\n",
    "rssi_bssids.extend(rssi_bssids_test)\n",
    "rssi_bssids_size = len(set(rssi_bssids))\n",
    "print(f'RSSI TYPES(all): {rssi_bssids_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文字列としてのsiteを残しておく\n",
    "train_df['site_id_str'] = train_df['site_id'].values\n",
    "test_df['site_id_str'] = test_df['site_id'].values\n",
    "\n",
    "# siteとfloorをconcat\n",
    "train_df['site_id'] = train_df['site_id'] + '-' +  train_df['floor'].astype(str)\n",
    "test_df['site_id'] = test_df['site_id'] + '-' + test_df['floor'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(wifi_bssids)\n",
    "le_site = LabelEncoder()\n",
    "le_site.fit(train_df['site_id'].unique())  # trainとtestでLE\n",
    "le_rssi = LabelEncoder()\n",
    "le_rssi.fit(rssi_bssids)\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(train_df.loc[:,RSSI_FEATS])\n",
    "\n",
    "\n",
    "def preprocess(input_df, le=le, le_site=le_site, ss=ss):\n",
    "    output_df = input_df.copy()\n",
    "    # RSSIの正規化\n",
    "    output_df.loc[:,RSSI_FEATS] = ss.transform(input_df.loc[:,RSSI_FEATS])\n",
    "\n",
    "    # BSSIDのLE\n",
    "    for i in BSSID_FEATS:\n",
    "        output_df.loc[:,i] = le.transform(input_df.loc[:,i])\n",
    "    for i in RSSI_FEATS:\n",
    "        output_df.loc[:,i] = le_rssi.transform(input_df.loc[:,i])\n",
    "\n",
    "    # site_id+floorのLE\n",
    "    output_df.loc[:, 'site_id'] = le_site.transform(input_df.loc[:, 'site_id'])\n",
    "\n",
    "    return output_df\n",
    "\n",
    "train = preprocess(train_df)\n",
    "test = preprocess(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "site_count = len(train['site_id'].unique())\n",
    "site_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch model\n",
    "- embedding layerが重要  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class IndoorDataset(Dataset):\n",
    "    def __init__(self, df, phase='train'):\n",
    "        self.df = df\n",
    "        self.phase = phase\n",
    "        self.bssid_feats = df[BSSID_FEATS].values.astype(int)\n",
    "        self.rssi_feats = df[RSSI_FEATS].values.astype(np.float32)\n",
    "        self.site_id = df['site_id'].values.astype(int)\n",
    "\n",
    "        if phase in ['train', 'valid']:\n",
    "            self.xy = df[['x', 'y']].values.astype(np.float32)\n",
    "            self.ixy = df[['ix', 'iy']].values.astype(np.float32)\n",
    "            self.floor = df['floor'].values.astype(np.float32)\n",
    "            timediff = df['timediff'].astype(np.float32).abs().values\n",
    "            self.weight = 1- (timediff/np.max(timediff))  # lossにかける重み。timediffが大きいほど小さくなる\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "    \n",
    "        concat_feat = np.empty(2 * NUM_FEATS).astype(int)\n",
    "        concat_feat[0::2] = self.bssid_feats[idx]\n",
    "        concat_feat[1::2] = self.rssi_feats[idx]\n",
    "        \n",
    "        feature = {\n",
    "            'RSSI_BSSID_FEATS':concat_feat,\n",
    "            'site_id':self.site_id[idx]\n",
    "        }\n",
    "        if self.phase in ['train', 'valid']:\n",
    "            target = {\n",
    "                'xy':self.xy[idx],\n",
    "                'ixy':self.ixy[idx],\n",
    "                'floor':self.floor[idx],\n",
    "                'weight': self.weight[idx]\n",
    "            }\n",
    "        else:\n",
    "            target = {}\n",
    "        return feature, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, wifi_bssids_size, site_count=24, embedding_dim=64):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        # bssid\n",
    "        # ->64次元に圧縮後sequence化にする\n",
    "        # wifi_bssids_sizeが辞書の数を表す\n",
    "        self.embedding_layer1 = nn.Sequential(\n",
    "            nn.Embedding(wifi_bssids_size, embedding_dim),\n",
    "            nn.Flatten(start_dim=-2)            \n",
    "        )\n",
    "        # site\n",
    "        # ->2次元に圧縮後sequence化する\n",
    "        # site_countが辞書の数を表す\n",
    "        self.embedding_layer2 = nn.Sequential(\n",
    "            nn.Embedding(site_count, 64),\n",
    "            nn.Flatten(start_dim=-1)           \n",
    "        )\n",
    "\n",
    "        # bssid, site, rssiの出力size\n",
    "        # feature_size = 2 + (2 * NUM_FEATS * embedding_dim)\n",
    "        feature_size = 64 + (2 * NUM_FEATS * 64)\n",
    "        self.linear_layer2 = nn.Sequential(\n",
    "            nn.BatchNorm1d(feature_size),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(feature_size, 256),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.batch_norm1 = nn.BatchNorm1d(1)\n",
    "        self.lstm1 = nn.LSTM(input_size=256,hidden_size=128,dropout=0.3, batch_first=True)\n",
    "        self.lstm2 = nn.LSTM(input_size=128,hidden_size=16,dropout=0.1, batch_first=True)\n",
    "\n",
    "        self.fc_xy = nn.Linear(16, 2)\n",
    "        self.fc_floor = nn.Linear(16, 1)\n",
    "\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # input embedding\n",
    "        batch_size = x[\"site_id\"].shape[0]\n",
    "        x_rssi_bssid = self.embedding_layer1(x['RSSI_BSSID_FEATS'])\n",
    "        x_site_id = self.embedding_layer2(x['site_id'])\n",
    "        # x_rssi = self.linear_layer1(x['RSSI_FEATS'])\n",
    "        x = torch.cat([x_rssi_bssid, x_site_id], dim=-1)\n",
    "        x = self.linear_layer2(x)\n",
    "\n",
    "        # lstm layer\n",
    "        x = x.view(batch_size, 1, -1)  # [batch, 1]->[batch, 1, 1]\n",
    "        x = self.batch_norm1(x)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = torch.relu(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = torch.relu(x)\n",
    "\n",
    "        # output [batch, 1, 1] -> [batch]\n",
    "        xy = self.fc_xy(x).squeeze(1)\n",
    "        floor = torch.relu(self.fc_floor(x)).view(-1)\n",
    "        return {\"xy\": xy, \"floor\": floor}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_position_error(xhat, yhat, fhat, x, y, f):\n",
    "    intermediate = np.sqrt(np.power(xhat-x, 2) + np.power(yhat-y, 2)) + 15 * np.abs(fhat-f)\n",
    "    return intermediate.sum()/xhat.shape[0]\n",
    "\n",
    "def to_np(input):\n",
    "    return input.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedMSELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.loss = nn.MSELoss(reduction='none')\n",
    "\n",
    "    def forward(self, input, target, weight):  # weight:timediffによる重みづけ\n",
    "        input = input.float()\n",
    "        target = target.float()\n",
    "        weight = torch.stack((weight, weight), 1).float()  # x,y分でstack\n",
    "        loss = self.loss(input, target) * weight  # timediffが大きいもののlossはあまり計算しないようにする\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer(model: nn.Module, config: dict):\n",
    "    optimizer_config = config[\"optimizer\"]\n",
    "    optimizer_name = optimizer_config.get(\"name\")\n",
    "    base_optimizer_name = optimizer_config.get(\"base_name\")\n",
    "    optimizer_params = optimizer_config['params']\n",
    "\n",
    "    if hasattr(optim, optimizer_name):\n",
    "        optimizer = optim.__getattribute__(optimizer_name)(model.parameters(), **optimizer_params)\n",
    "        return optimizer\n",
    "    else:\n",
    "        base_optimizer = optim.__getattribute__(base_optimizer_name)\n",
    "        optimizer = globals().get(optimizer_name)(\n",
    "            model.parameters(), \n",
    "            base_optimizer,\n",
    "            **optimizer_config[\"params\"])\n",
    "        return  optimizer\n",
    "\n",
    "def get_scheduler(optimizer, config: dict):\n",
    "    scheduler_config = config[\"scheduler\"]\n",
    "    scheduler_name = scheduler_config.get(\"name\")\n",
    "\n",
    "    if scheduler_name is None:\n",
    "        return\n",
    "    else:\n",
    "        return optim.lr_scheduler.__getattribute__(scheduler_name)(\n",
    "            optimizer, **scheduler_config[\"params\"])\n",
    "\n",
    "\n",
    "def get_criterion(config: dict):\n",
    "    loss_config = config[\"loss\"]\n",
    "    loss_name = loss_config[\"name\"]\n",
    "    loss_params = {} if loss_config.get(\"params\") is None else loss_config.get(\"params\")\n",
    "    \n",
    "    \n",
    "    if hasattr(nn, loss_name):\n",
    "        criterion = nn.__getattribute__(loss_name)(**loss_params)\n",
    "    else:\n",
    "        criterion = globals().get(loss_name)(**loss_params)\n",
    "\n",
    "    return criterion\n",
    "\n",
    "def worker_init_fn(worker_id):                                                          \n",
    "    np.random.seed(np.random.get_state()[1][0] + worker_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learner class(pytorch-lighting)\n",
    "class Learner(pl.LightningModule):\n",
    "    def __init__(self, model, config):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.config = config\n",
    "        self.xy_criterion = get_criterion(config)\n",
    "        self.f_criterion = nn.MSELoss()\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "\n",
    "        output = self.model(x)\n",
    "        xy_loss = self.xy_criterion(output[\"xy\"], y[\"ixy\"], y['weight'])\n",
    "        floor_loss = self.f_criterion(output[\"floor\"], y[\"floor\"])\n",
    "        loss = xy_loss + floor_loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        output = self.model(x)\n",
    "        xy_loss = self.xy_criterion(output[\"xy\"], y[\"ixy\"], y['weight'])\n",
    "        f_loss = self.f_criterion(output[\"floor\"], y[\"floor\"])\n",
    "        loss = xy_loss + f_loss\n",
    "        mpe = mean_position_error(\n",
    "            to_np(output['xy'][:, 0]), to_np(output['xy'][:, 1]), 0, \n",
    "            to_np(y['xy'][:, 0]), to_np(y['xy'][:, 1]), 0)\n",
    "\n",
    "        impe = mean_position_error(\n",
    "            to_np(output['xy'][:, 0]), to_np(output['xy'][:, 1]), 0, \n",
    "            to_np(y['ixy'][:, 0]), to_np(y['ixy'][:, 1]), 0)\n",
    "\n",
    "        # floor lossは現状は無視して良い\n",
    "        self.log(f'Loss/val', loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'Loss/xy', xy_loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'Loss/floor', f_loss, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'MPE/val', mpe, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        self.log(f'iMPE/val', impe, on_step=False, on_epoch=True, prog_bar=False, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = get_optimizer(self.model, self.config)\n",
    "        scheduler = get_scheduler(optimizer, self.config)\n",
    "        return {\"optimizer\": optimizer, \"lr_scheduler\": scheduler, \"monitor\": \"Loss/val\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oof\n",
    "def evaluate(model, loaders, phase):\n",
    "    x_list = []\n",
    "    y_list = []\n",
    "    f_list = []\n",
    "    with torch.no_grad():\n",
    "        for batch in loaders[phase]:\n",
    "            x, y = batch\n",
    "            output = model(x)\n",
    "            x_list.append(to_np(output['xy'][:, 0]))\n",
    "            y_list.append(to_np(output['xy'][:, 1]))\n",
    "            f_list.append(to_np(output['floor']))\n",
    "\n",
    "    x_list = np.concatenate(x_list)\n",
    "    y_list = np.concatenate(y_list)\n",
    "    f_list = np.concatenate(f_list)\n",
    "    return x_list, y_list, f_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================\n",
      "Fold 0\n",
      "====================\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "/usr/local/lib/python3.8/dist-packages/torch/nn/modules/rnn.py:58: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkuto5046\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.22<br/>\n                Syncing run <strong style=\"color:#cdcd00\">northern-bee-1239</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/kuto5046/indoor\" target=\"_blank\">https://wandb.ai/kuto5046/indoor</a><br/>\n                Run page: <a href=\"https://wandb.ai/kuto5046/indoor/runs/1i3jjxwa\" target=\"_blank\">https://wandb.ai/kuto5046/indoor/runs/1i3jjxwa</a><br/>\n                Run data is saved locally in <code>/home/user/work/exp/exp132/wandb/run-20210516_154223-1i3jjxwa</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name         | Type            | Params\n",
      "-------------------------------------------------\n",
      "0 | model        | LSTMModel       | 6.8 M \n",
      "1 | xy_criterion | WeightedMSELoss | 0     \n",
      "2 | f_criterion  | MSELoss         | 0     \n",
      "-------------------------------------------------\n",
      "6.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.8 M     Total params\n",
      "27.028    Total estimated model params size (MB)\n",
      "fold 0: mean position error 7.397075337439683\n",
      "fold 0: mean position error 7.101450101176313\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 610572<br/>Program ended successfully."
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d72b96227ecc4b11812659e535c56077"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>/home/user/work/exp/exp132/wandb/run-20210516_154223-1i3jjxwa/logs/debug.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>/home/user/work/exp/exp132/wandb/run-20210516_154223-1i3jjxwa/logs/debug-internal.log</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>Loss/val</td><td>43.29226</td></tr><tr><td>Loss/xy</td><td>43.02258</td></tr><tr><td>Loss/floor</td><td>0.26967</td></tr><tr><td>MPE/val</td><td>7.39691</td></tr><tr><td>iMPE/val</td><td>7.10124</td></tr><tr><td>epoch</td><td>114</td></tr><tr><td>trainer/global_step</td><td>80844</td></tr><tr><td>_runtime</td><td>801</td></tr><tr><td>_timestamp</td><td>1621148144</td></tr><tr><td>_step</td><td>114</td></tr></table>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>Loss/val</td><td>█▅▅▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Loss/xy</td><td>█▅▅▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Loss/floor</td><td>█████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>MPE/val</td><td>█▆▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>iMPE/val</td><td>█▆▆▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">northern-bee-1239</strong>: <a href=\"https://wandb.ai/kuto5046/indoor/runs/1i3jjxwa\" target=\"_blank\">https://wandb.ai/kuto5046/indoor/runs/1i3jjxwa</a><br/>\n                "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "====================\n",
      "Fold 1\n",
      "====================\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.30 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.22<br/>\n                Syncing run <strong style=\"color:#cdcd00\">eager-sky-1240</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/kuto5046/indoor\" target=\"_blank\">https://wandb.ai/kuto5046/indoor</a><br/>\n                Run page: <a href=\"https://wandb.ai/kuto5046/indoor/runs/39mslooc\" target=\"_blank\">https://wandb.ai/kuto5046/indoor/runs/39mslooc</a><br/>\n                Run data is saved locally in <code>/home/user/work/exp/exp132/wandb/run-20210516_155559-39mslooc</code><br/><br/>\n            "
     },
     "metadata": {}
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "\n",
      "  | Name         | Type            | Params\n",
      "-------------------------------------------------\n",
      "0 | model        | LSTMModel       | 6.8 M \n",
      "1 | xy_criterion | WeightedMSELoss | 0     \n",
      "2 | f_criterion  | MSELoss         | 0     \n",
      "-------------------------------------------------\n",
      "6.8 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.8 M     Total params\n",
      "27.028    Total estimated model params size (MB)\n",
      "Exception ignored in: <function _releaseLock at 0x7f75d5c2e280>\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.8/logging/__init__.py\", line 223, in _releaseLock\n",
      "    def _releaseLock():\n",
      "KeyboardInterrupt: \n",
      "/usr/local/lib/python3.8/dist-packages/pytorch_lightning/utilities/distributed.py:50: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "Exception",
     "evalue": "The wandb backend process has shutdown",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-9ebad5497b5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    106\u001b[0m         oof_df['x'].values, oof_df['y'].values, 0)\n\u001b[1;32m    107\u001b[0m     \u001b[0mval_scores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"fold {fold}: mean position error {val_score}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# 補間後のtargetで評価\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/wandb/sdk/lib/redirect.py\u001b[0m in \u001b[0;36mnew_write\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mnew_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m                 \u001b[0mcb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m                 \u001b[0mold_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/wandb/sdk/wandb_run.py\u001b[0m in \u001b[0;36m_console_callback\u001b[0;34m(self, name, data)\u001b[0m\n\u001b[1;32m    794\u001b[0m         \u001b[0;31m# logger.info(\"console callback: %s, %s\", name, data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpublish_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m     def _tensorboard_callback(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36mpublish_output\u001b[0;34m(self, name, data)\u001b[0m\n\u001b[1;32m    185\u001b[0m         \u001b[0mo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutputRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0motype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0mo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGetCurrentTime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutdata\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutputRecord\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36m_publish_output\u001b[0;34m(self, outdata)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecord\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mrec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     def publish_tbdata(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/wandb/sdk/interface/interface.py\u001b[0m in \u001b[0;36m_publish\u001b[0;34m(self, record, local)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_publish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRecord\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The wandb backend process has shutdown\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mException\u001b[0m: The wandb backend process has shutdown"
     ]
    }
   ],
   "source": [
    "oofs = []  # 全てのoofをdfで格納する\n",
    "predictions = []  # 全ての予測値をdfで格納する\n",
    "val_scores = []\n",
    "# skf = model_selection.StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n",
    "gkf = model_selection.GroupKFold(n_splits=N_SPLITS)\n",
    "\n",
    "train_fold = [(trn_idx, val_idx) for trn_idx, val_idx in gkf.split(train.loc[:, 'path'], groups=train.loc[:, 'path'])]\n",
    "# 今回はtargetを均等に分ける必要はなくpathが均等に分かれればいいのでskf.split()にpathを与えている。\n",
    "for fold in range(5):\n",
    "    # 指定したfoldのみループを回す\n",
    "    if fold not in USE_FOLDS:\n",
    "        continue\n",
    "\n",
    "    print('=' * 20)\n",
    "    print(f'Fold {fold}')\n",
    "    print('=' * 20)\n",
    "\n",
    "    # train/valid data\n",
    "    trn_idx_for_train, val_idx_for_train = train_fold[fold]\n",
    "    trn_df = train.loc[trn_idx_for_train, :].reset_index(drop=True)\n",
    "    trn_df = trn_df.loc[:,BSSID_FEATS + RSSI_FEATS + ['site_id', 'x', 'y', 'ix','iy','floor','timediff']]\n",
    "    \n",
    "    val_df = train.loc[val_idx_for_train, :].reset_index(drop=True)\n",
    "    val_df = val_df.loc[:,BSSID_FEATS + RSSI_FEATS + ['site_id', 'x', 'y', 'ix','iy','floor','timediff']]\n",
    "\n",
    "    # data loader\n",
    "    loaders = {}\n",
    "    loader_config = config[\"loader\"]\n",
    "    loaders[\"train\"] = DataLoader(IndoorDataset(trn_df, phase=\"train\"), **loader_config[\"train\"], worker_init_fn=worker_init_fn) \n",
    "    loaders[\"valid\"] = DataLoader(IndoorDataset(val_df, phase=\"valid\"), **loader_config[\"valid\"], worker_init_fn=worker_init_fn)\n",
    "    loaders[\"test\"] = DataLoader(IndoorDataset(test, phase=\"test\"), **loader_config[\"test\"], worker_init_fn=worker_init_fn)\n",
    "\n",
    "    # model\n",
    "    model = LSTMModel(wifi_bssids_size, site_count)  # +1としているのはLEを1スタートで始めているため\n",
    "    model_name = model.__class__.__name__\n",
    "\n",
    "    # callbacks\n",
    "    callbacks = []\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=f'Loss/val',\n",
    "        mode='min',\n",
    "        dirpath=f\"../../model/{EXP_NAME}\",\n",
    "        verbose=False,\n",
    "        filename=f'{model_name}-{fold}')\n",
    "    \n",
    "    if MODEL_SAVE:\n",
    "        callbacks.append(checkpoint_callback)\n",
    "\n",
    "    early_stop_callback = EarlyStopping(\n",
    "        monitor='Loss/val',\n",
    "        min_delta=0.00,\n",
    "        patience=30,\n",
    "        verbose=False,\n",
    "        mode='min')\n",
    "    callbacks.append(early_stop_callback)\n",
    "\n",
    "    # loggers\n",
    "    RUN_NAME = EXP_NAME + \"_\" + EXP_MESSAGE\n",
    "    wandb.init(project='indoor', notes=NOTES, entity='kuto5046', group=RUN_NAME)\n",
    "    wandb.run.name = RUN_NAME + f'-fold-{fold}'\n",
    "    wandb_config = wandb.config\n",
    "    wandb_config['model_name'] = model_name\n",
    "    wandb_config['LB'] = None\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    \n",
    "    loggers = []\n",
    "    loggers.append(WandbLogger())\n",
    "\n",
    "    learner = Learner(model, config)\n",
    "    # pretrained flag\n",
    "    if PRETRAINED:\n",
    "        ckpt = torch.load(PRETRAINED_PATH + f'{model_name}-{fold}.ckpt')\n",
    "        learner.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "    if not ONLY_PRED:\n",
    "        trainer = pl.Trainer(\n",
    "            logger=loggers, \n",
    "            callbacks=callbacks,\n",
    "            max_epochs=MAX_EPOCHS,\n",
    "            gpus=[0],\n",
    "            fast_dev_run=DEBUG,\n",
    "            deterministic=True,\n",
    "            # precision=16,\n",
    "            progress_bar_refresh_rate=0  # vscodeの時progress barの動作が遅いので表示しない\n",
    "            )\n",
    "\n",
    "        trainer.fit(learner, train_dataloader=loaders['train'], val_dataloaders=loaders['valid'])\n",
    "\n",
    "    #############\n",
    "    # validation (to make oof)\n",
    "    #############\n",
    "\n",
    "    model.eval() \n",
    "    oof_df = train.loc[val_idx_for_train, RSSI_FEATS + TIMEDIFF_FEATS + ['id', 'timestamp', 'x', 'y', 'floor', 'ix', 'iy', 'floor_str','path', 'site_id', 'site_id_str']].reset_index(drop=True)\n",
    "    # oof_df = train.loc[val_idx_for_train, ['id', 'timestamp', 'x', 'y', 'floor', 'ix', 'iy', 'floor_str','path', 'site_id', 'site_id_str']].reset_index(drop=True)\n",
    "    oof_x, oof_y, oof_f = evaluate(model, loaders, phase=\"valid\")\n",
    "    oof_df[\"oof_x\"] = oof_x\n",
    "    oof_df[\"oof_y\"] = oof_y\n",
    "    oof_df[\"oof_floor\"] = oof_f\n",
    "    oofs.append(oof_df)\n",
    "    \n",
    "    # 補間前のtargetで評価\n",
    "    val_score = mean_position_error(\n",
    "        oof_df[\"oof_x\"].values, oof_df[\"oof_y\"].values, 0,\n",
    "        oof_df['x'].values, oof_df['y'].values, 0)\n",
    "    val_scores.append(val_score)\n",
    "    print(f\"fold {fold}: mean position error {val_score}\")\n",
    "\n",
    "    # 補間後のtargetで評価\n",
    "    val_score = mean_position_error(\n",
    "        oof_df[\"oof_x\"].values, oof_df[\"oof_y\"].values, 0,\n",
    "        oof_df['ix'].values, oof_df['iy'].values, 0)\n",
    "    val_scores.append(val_score)\n",
    "    print(f\"fold {fold}: mean position error {val_score}\")\n",
    "\n",
    "    #############\n",
    "    # inference\n",
    "    #############\n",
    "\n",
    "    preds_x, preds_y, preds_f = evaluate(model, loaders, phase=\"test\")\n",
    "    test_preds = pd.DataFrame(np.stack((preds_f, preds_x, preds_y))).T\n",
    "    test_preds.columns = sub_df.columns\n",
    "    test_preds[\"site_path_timestamp\"] = test[\"site_path_timestamp\"]\n",
    "    test_preds[\"floor\"] = test_preds[\"floor\"].astype(int)\n",
    "    # test_preds.to_csv(f'{EXP_NAME}_fold{fold}.csv', index=False)\n",
    "    predictions.append(test_preds)\n",
    "\n",
    "    if fold != 4:\n",
    "        wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(USE_FOLDS) > 1:\n",
    "    oofs_df = pd.concat(oofs)\n",
    "else:\n",
    "    oofs_df = oofs[0]\n",
    "\n",
    "oofs_df['site_path_timestamp'] = oofs_df['site_id_str'].astype(str) + '_' + oofs_df['path'] + '_' + oofs_df['timestamp'].astype(str)\n",
    "oofs_df = oofs_df.sort_values('site_path_timestamp').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oofs_df[['id', 'site_path_timestamp', 'oof_x', 'oof_y']].sort_values('id').to_csv('oof.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平均をとる\n",
    "_oofs_df = oofs_df.groupby(\"site_path_timestamp\").mean().reset_index()\n",
    "_oofs_df[\"site_id_str\"] = _oofs_df[\"site_path_timestamp\"].str.split(\"_\", expand=True)[0]\n",
    "_oofs_df[\"path\"] = _oofs_df[\"site_path_timestamp\"].str.split(\"_\", expand=True)[1]\n",
    "oofs_df = pd.merge(_oofs_df, oofs_df[[\"path\", \"floor_str\"]].drop_duplicates(), on=\"path\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waypoint補正前のx,yでの評価\n",
    "oof_score = mean_position_error(\n",
    "    oofs_df['oof_x'], oofs_df['oof_y'], 0, \n",
    "    oofs_df['x'], oofs_df['y'], 0\n",
    "    )\n",
    "wandb_config['CV'] = oof_score\n",
    "print(f\"CV:{oof_score}\")"
   ]
  },
  {
   "source": [
    "## subの用意"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(USE_FOLDS) > 1:\n",
    "    # foldの結果を平均した後、reindexでsubmission fileにindexを合わせる\n",
    "    # all_preds = pd.concat(predictions).groupby('site_path_timestamp').mean()\n",
    "    all_preds = pd.concat(predictions).reset_index().groupby('index').mean()\n",
    "    all_preds['site_path_timestamp'] = predictions[0]['site_path_timestamp'].values\n",
    "else:\n",
    "    all_preds = predictions[0]\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = all_preds.groupby('site_path_timestamp').mean()\n",
    "all_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds_20s = pd.read_csv('../../exp/exp114/exp114_cost_snap_sub.csv', index_col=0)\n",
    "\n",
    "# オリジナルのsite_path_timestampをもとにmerge\n",
    "sub = pd.merge(all_preds_20s, all_preds, how='left', on='site_path_timestamp')[['floor_y', 'x_y', 'y_y']]\n",
    "sub = sub.rename(columns={'floor_y': 'floor', 'x_y': 'x', 'y_y': 'y'})\n",
    "\n",
    "# 欠損している箇所は補間\n",
    "sub['floor'].fillna(all_preds_20s['floor'], inplace=True)\n",
    "sub['x'].fillna(all_preds_20s['x'].astype(np.float32), inplace=True)\n",
    "sub['y'].fillna(all_preds_20s['y'].astype(np.float32), inplace=True)\n",
    "\n",
    "# foldの結果を平均した後、reindexでsubmission fileにindexを合わせる\n",
    "sub.index = sub_df.index\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # floorの数値を置換\n",
    "# simple_accurate_99 = pd.read_csv(root_dir / 'simple-99-accurate-floor-model/submission.csv')\n",
    "# sub['floor'] = simple_accurate_99['floor'].values\n",
    "floor_sub = pd.read_csv(root_dir / 'base_lb3.727_BiLSTM_skf_cv999.csv')  # しんちろさんのsub\n",
    "sub['floor'] = floor_sub['floor'].values\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(EXP_NAME + '_sub.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 後処理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cost minimaization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import scipy.interpolate\n",
    "import scipy.sparse\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "from src.io_f import read_data_file, read_data_file_for_leak\n",
    "from src import compute_f\n",
    "from scipy.signal import butter, lfilter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "order = 3\n",
    "fs = 50.0  # sample rate, Hz\n",
    "# fs = 100\n",
    "# cutoff = 3.667  # desired cutoff frequency of the filter, Hz\n",
    "cutoff = 3\n",
    "\n",
    "step_distance = 0.8\n",
    "w_height = 1.7\n",
    "m_trans = -5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickle_load(path):\n",
    "    with open(path, mode='rb') as f:\n",
    "        data = pickle.load(f)\n",
    "        return data\n",
    "\n",
    "# delta_x_coeff_site = pickle_load('../exp046/delta_magn_x_coeff.pkl')\n",
    "# delta_y_coeff_site = pickle_load('../exp046/delta_magn_y_coeff.pkl')\n",
    "delta_x_coeff_floor = pickle_load('../exp056/delta_magn_x_coeff_floor.pkl')\n",
    "delta_y_coeff_floor = pickle_load('../exp056/delta_magn_y_coeff_floor.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rel_positions(acce_datas, ahrs_datas):\n",
    "    step_timestamps, step_indexs, step_acce_max_mins = compute_f.compute_steps(acce_datas)\n",
    "    headings = compute_f.compute_headings(ahrs_datas)\n",
    "    stride_lengths = compute_f.compute_stride_length(step_acce_max_mins)\n",
    "    step_headings = compute_f.compute_step_heading(step_timestamps, headings)\n",
    "    rel_positions = compute_f.compute_rel_positions(stride_lengths, step_headings)\n",
    "    return rel_positions\n",
    "\n",
    "\n",
    "def correct_path(args):\n",
    "    path, path_df = args\n",
    "    \n",
    "    T_ref  = path_df['timestamp'].values\n",
    "    xy_hat = path_df[['x', 'y']].values\n",
    "    example = read_data_file(root_dir/ f'indoor-location-navigation/test/{path}.txt')\n",
    "    \n",
    "    # rel_positions = compute_rel_positions(example.acce, example.ahrs)\n",
    "    # ↑を↓に置き換える\n",
    "    rel_positions1 = compute_rel_positions(example.acce, example.ahrs)\n",
    "    rel_positions2 = steps_compute_rel_positions(example)\n",
    "    rel1 = rel_positions1.copy()\n",
    "    rel2 = rel_positions2.copy()\n",
    "    rel1[:,1:] = rel_positions1[:,1:] / 2\n",
    "    rel2[:,1:] = rel_positions2[:,1:] / 2\n",
    "    rel_positions = np.vstack([rel1,rel2])\n",
    "    rel_positions = rel_positions[np.argsort(rel_positions[:, 0])]\n",
    "    \n",
    "    if T_ref[-1] > rel_positions[-1, 0]:\n",
    "        rel_positions = [np.array([[0, 0, 0]]), rel_positions, np.array([[T_ref[-1], 0, 0]])]\n",
    "    else:\n",
    "        rel_positions = [np.array([[0, 0, 0]]), rel_positions]\n",
    "    rel_positions = np.concatenate(rel_positions)\n",
    "    \n",
    "    # 相対位置に補正係数をかける\n",
    "    site_id = path_df['site'].unique()[0]\n",
    "    floor = int(path_df['floor'].unique()[0])\n",
    "\n",
    "    # 辞書にsite-floorの組み合わせが存在していれば\n",
    "    coeff_x = delta_x_coeff_floor['a'][f'{site_id}_{floor}'] \n",
    "    coeff_y = delta_y_coeff_floor['a'][f'{site_id}_{floor}'] \n",
    "    rel_positions[:, 1] = rel_positions[:, 1]*coeff_x  # ∆xの補正\n",
    "    rel_positions[:, 2] = rel_positions[:, 2]*coeff_y  # ∆yの補正\n",
    "\n",
    "    T_rel = rel_positions[:, 0]\n",
    "    delta_xy_hat = np.diff(scipy.interpolate.interp1d(T_rel, np.cumsum(rel_positions[:, 1:3], axis=0), axis=0)(T_ref), axis=0)\n",
    "\n",
    "    N = xy_hat.shape[0]\n",
    "    delta_t = np.diff(T_ref)\n",
    "    alpha = (8.1)**(-2) * np.ones(N)\n",
    "    beta  = (0.3 + 0.3 * 1e-3 * delta_t)**(-2)\n",
    "    A = scipy.sparse.spdiags(alpha, [0], N, N)\n",
    "    B = scipy.sparse.spdiags( beta, [0], N-1, N-1)\n",
    "    D = scipy.sparse.spdiags(np.stack([-np.ones(N), np.ones(N)]), [0, 1], N-1, N)\n",
    "\n",
    "    Q = A + (D.T @ B @ D)\n",
    "    c = (A @ xy_hat) + (D.T @ (B @ delta_xy_hat))\n",
    "    xy_star = scipy.sparse.linalg.spsolve(Q, c)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'site_path_timestamp' : path_df['site_path_timestamp'],\n",
    "        'floor' : path_df['floor'],\n",
    "        'x' : xy_star[:, 0],\n",
    "        'y' : xy_star[:, 1],\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "def correct_path_train(args):\n",
    "    #print(args)\n",
    "    (site_id, path, floor, floor_str), path_df = args\n",
    "    \n",
    "    T_ref  = path_df['timestamp'].values\n",
    "    xy_hat = path_df[['x', 'y']].values\n",
    "    \n",
    "    example = read_data_file(f'{root_dir}/indoor-location-navigation/train/{site_id}/{floor_str}/{path}.txt')\n",
    "    \n",
    "    # rel_positions = compute_rel_positions(example.acce, example.ahrs)\n",
    "    # ↑を↓に置き換える\n",
    "    rel_positions1 = compute_rel_positions(example.acce, example.ahrs)\n",
    "    rel_positions2 = steps_compute_rel_positions(example)\n",
    "    rel1 = rel_positions1.copy()\n",
    "    rel2 = rel_positions2.copy()\n",
    "    rel1[:,1:] = rel_positions1[:,1:] / 2\n",
    "    rel2[:,1:] = rel_positions2[:,1:] / 2\n",
    "    rel_positions = np.vstack([rel1,rel2])\n",
    "    rel_positions = rel_positions[np.argsort(rel_positions[:, 0])]\n",
    "    \n",
    "    \n",
    "    if T_ref[-1] > rel_positions[-1, 0]:\n",
    "        rel_positions = [np.array([[0, 0, 0]]), rel_positions, np.array([[T_ref[-1], 0, 0]])]\n",
    "    else:\n",
    "        rel_positions = [np.array([[0, 0, 0]]), rel_positions]\n",
    "    rel_positions = np.concatenate(rel_positions)\n",
    "    \n",
    "    # 相対位置に補正係数をかける\n",
    "    # 辞書にsite-floorの組み合わせが存在していれば\n",
    "    floor = int(floor)\n",
    "    coeff_x = delta_x_coeff_floor['a'][f'{site_id}_{floor}'] \n",
    "    coeff_y = delta_y_coeff_floor['a'][f'{site_id}_{floor}'] \n",
    "    rel_positions[:, 1] = rel_positions[:, 1]*coeff_x  # ∆xの補正\n",
    "    rel_positions[:, 2] = rel_positions[:, 2]*coeff_y  # ∆yの補正\n",
    "\n",
    "    T_rel = rel_positions[:, 0]\n",
    "\n",
    "    try:\n",
    "        delta_xy_hat = np.diff(scipy.interpolate.interp1d(T_rel, np.cumsum(rel_positions[:, 1:3], axis=0), axis=0)(T_ref), axis=0)\n",
    "    except:\n",
    "        return pd.DataFrame({\n",
    "            'site_path_timestamp' : path_df['site_path_timestamp'],\n",
    "            'floor' : path_df['floor'],\n",
    "            'x' : path_df['x'].to_numpy(),\n",
    "            'y' : path_df['y'].to_numpy()\n",
    "        })\n",
    "    \n",
    "\n",
    "    N = xy_hat.shape[0]\n",
    "    delta_t = np.diff(T_ref)\n",
    "    alpha = (8.1)**(-2) * np.ones(N)\n",
    "    beta  = (0.3 + 0.3 * 1e-3 * delta_t)**(-2)\n",
    "    A = scipy.sparse.spdiags(alpha, [0], N, N)\n",
    "    B = scipy.sparse.spdiags( beta, [0], N-1, N-1)\n",
    "    D = scipy.sparse.spdiags(np.stack([-np.ones(N), np.ones(N)]), [0, 1], N-1, N)\n",
    "\n",
    "    Q = A + (D.T @ B @ D)\n",
    "    c = (A @ xy_hat) + (D.T @ (B @ delta_xy_hat))\n",
    "    xy_star = scipy.sparse.linalg.spsolve(Q, c)\n",
    "\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        'site_path_timestamp' : path_df['site_path_timestamp'],\n",
    "        'floor' : path_df['floor'],\n",
    "        'x' : xy_star[:, 0],\n",
    "        'y' : xy_star[:, 1],\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "def butter_lowpass(cutoff, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    normal_cutoff = cutoff / nyq\n",
    "    b, a = butter(order, normal_cutoff, btype='low', analog=False)\n",
    "    return b, a\n",
    "\n",
    "def butter_lowpass_filter(data, cutoff, fs, order=5):\n",
    "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
    "    y = lfilter(b, a, data)\n",
    "    return y\n",
    "\n",
    "def peak_accel_threshold(data, timestamps, threshold):\n",
    "    d_acc = []\n",
    "    last_state = 'below'\n",
    "    crest_troughs = 0\n",
    "    crossings = []\n",
    "\n",
    "    for i, datum in enumerate(data):\n",
    "        \n",
    "        current_state = last_state\n",
    "        if datum < threshold:\n",
    "            current_state = 'below'\n",
    "        elif datum > threshold:\n",
    "            current_state = 'above'\n",
    "\n",
    "        if current_state is not last_state:\n",
    "            if current_state is 'above':\n",
    "                crossing = [timestamps[i], threshold]\n",
    "                crossings.append(crossing)\n",
    "            else:\n",
    "                crossing = [timestamps[i], threshold]\n",
    "                crossings.append(crossing)\n",
    "\n",
    "            crest_troughs += 1\n",
    "        last_state = current_state\n",
    "    return np.array(crossings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def steps_compute_rel_positions(sample_file):\n",
    "    \n",
    "    mix_acce = np.sqrt(sample_file.acce[:,1:2]**2 + sample_file.acce[:,2:3]**2 + sample_file.acce[:,3:4]**2)\n",
    "    mix_acce = np.concatenate([sample_file.acce[:,0:1], mix_acce], 1)\n",
    "    mix_df = pd.DataFrame(mix_acce)\n",
    "    mix_df.columns = [\"timestamp\",\"acce\"]\n",
    "    \n",
    "    filtered = butter_lowpass_filter(mix_df[\"acce\"], cutoff, fs, order)\n",
    "\n",
    "    threshold = filtered.mean() * 1.1\n",
    "    crossings = peak_accel_threshold(filtered, mix_df[\"timestamp\"], threshold)\n",
    "\n",
    "    step_sum = len(crossings)/2\n",
    "    distance = w_height * 0.4 * step_sum\n",
    "\n",
    "    mag_df = pd.DataFrame(sample_file.magn)\n",
    "    mag_df.columns = [\"timestamp\",\"x\",\"y\",\"z\"]\n",
    "    \n",
    "    acce_df = pd.DataFrame(sample_file.acce)\n",
    "    acce_df.columns = [\"timestamp\",\"ax\",\"ay\",\"az\"]\n",
    "    \n",
    "    mag_df = pd.merge(mag_df,acce_df,on=\"timestamp\")\n",
    "    mag_df.dropna()\n",
    "    \n",
    "    time_di_list = []\n",
    "\n",
    "    for i in mag_df.iterrows():\n",
    "\n",
    "        gx,gy,gz = i[1][1],i[1][2],i[1][3]\n",
    "        ax,ay,az = i[1][4],i[1][5],i[1][6]\n",
    "\n",
    "        roll = math.atan2(ay,az)\n",
    "        pitch = math.atan2(-1*ax , (ay * math.sin(roll) + az * math.cos(roll)))\n",
    "\n",
    "        q = m_trans - math.degrees(math.atan2(\n",
    "            (gz*math.sin(roll)-gy*math.cos(roll)),(gx*math.cos(pitch) + gy*math.sin(roll)*math.sin(pitch) + gz*math.sin(pitch)*math.cos(roll))\n",
    "        )) -90\n",
    "        if q <= 0:\n",
    "            q += 360\n",
    "        time_di_list.append((i[1][0],q))\n",
    "\n",
    "    d_list = [x[1] for x in time_di_list]\n",
    "    \n",
    "    steps = []\n",
    "    step_time = []\n",
    "    di_dict = dict(time_di_list)\n",
    "\n",
    "    for n,i in enumerate(crossings[:,:1]):\n",
    "        if n % 2 == 1:\n",
    "            continue\n",
    "        direct_now = di_dict[i[0]]\n",
    "        dx = math.sin(math.radians(direct_now))\n",
    "        dy = math.cos(math.radians(direct_now))\n",
    "#         print(int(n/2+1),\"歩目/x:\",dx,\"/y:\",dy,\"/角度：\",direct_now)\n",
    "        steps.append((i[0],dx,dy))\n",
    "        step_time.append(i[0])\n",
    "    \n",
    "        step_dtime = np.diff(step_time)/1000\n",
    "        step_dtime = step_dtime.tolist()\n",
    "        step_dtime.insert(0,5)\n",
    "        \n",
    "        rel_position = []\n",
    "\n",
    "        wp_idx = 0\n",
    "#         print(\"WP:\",round(sample_file.waypoint[0,1],3),round(sample_file.waypoint[0,2],3),sample_file.waypoint[0,0])\n",
    "#         print(\"------------------\")\n",
    "        for p,i in enumerate(steps):\n",
    "            step_distance = 0\n",
    "            if step_dtime[p] >= 1:\n",
    "                step_distance = w_height*0.25\n",
    "            elif step_dtime[p] >= 0.75:\n",
    "                step_distance = w_height*0.3\n",
    "            elif step_dtime[p] >= 0.5:\n",
    "                step_distance = w_height*0.4\n",
    "            elif step_dtime[p] >= 0.35:\n",
    "                step_distance = w_height*0.45\n",
    "            elif step_dtime[p] >= 0.2:\n",
    "                step_distance = w_height*0.5\n",
    "            else:\n",
    "                step_distance = w_height*0.4\n",
    "\n",
    "#             step_x += i[1]*step_distance\n",
    "#             step_y += i[2]*step_distance\n",
    "            \n",
    "            rel_position.append([i[0], i[1]*step_distance, i[2]*step_distance])\n",
    "#     print(rel_position)\n",
    "    \n",
    "    return np.array(rel_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## snap to grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "def sub_process(sub, train_waypoints):\n",
    "    train_waypoints['isTrainWaypoint'] = True\n",
    "    sub = split_col(sub[['site_path_timestamp','floor','x','y']]).copy()\n",
    "    sub = sub.merge(train_waypoints[['site','floor']].drop_duplicates(), how='left')\n",
    "    sub = sub.merge(\n",
    "        train_waypoints[['x','y','site','floor','isTrainWaypoint']].drop_duplicates(),\n",
    "        how='left',\n",
    "        on=['site','x','y','floor']\n",
    "             )\n",
    "    sub['isTrainWaypoint'] = sub['isTrainWaypoint'].fillna(False)\n",
    "    return sub.copy()\n",
    "\n",
    "def split_col(df):\n",
    "    df = pd.concat([\n",
    "        df['site_path_timestamp'].str.split('_', expand=True) \\\n",
    "        .rename(columns={0:'site',\n",
    "                         1:'path',\n",
    "                         2:'timestamp'}),\n",
    "        df\n",
    "    ], axis=1).copy()\n",
    "    return df\n",
    "\n",
    "def add_xy(df):\n",
    "    df['xy'] = [(x, y) for x,y in zip(df['x'], df['y'])]\n",
    "    return df\n",
    "\n",
    "def closest_point(point, points):\n",
    "    \"\"\" Find closest point from a list of points. \"\"\"\n",
    "    return points[cdist([point], points).argmin()]\n",
    "\n",
    "def snap_to_grid(sub, threshold=5):\n",
    "    \"\"\"\n",
    "    Snap to grid if within a threshold.\n",
    "    \n",
    "    x, y are the predicted points.\n",
    "    x_, y_ are the closest grid points.\n",
    "    _x_, _y_ are the new predictions after post processing.\n",
    "    \"\"\"\n",
    "    sub['_x_'] = sub['x']\n",
    "    sub['_y_'] = sub['y']\n",
    "    sub.loc[sub['dist'] < threshold, '_x_'] = sub.loc[sub['dist'] < threshold]['x_']\n",
    "    sub.loc[sub['dist'] < threshold, '_y_'] = sub.loc[sub['dist'] < threshold]['y_']\n",
    "    return sub.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing_for_train(oofs_df, n):    \n",
    "    ###############\n",
    "    # cost min\n",
    "    ###############\n",
    "    print(f\"\\n{n+1}回目\")\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(correct_path_train, oofs_df.groupby(['site_id_str', 'path', 'floor', 'floor_str']))\n",
    "        dfs = tqdm(dfs)\n",
    "        dfs = list(dfs)\n",
    "\n",
    "    oof_post_process = pd.concat(dfs).sort_index()\n",
    "    oofs_df['oof_min_x'] = oof_post_process['x']\n",
    "    oofs_df['oof_min_y'] = oof_post_process['y']\n",
    "\n",
    "    # 元に戻す\n",
    "    oofs_df = oofs_df.rename(columns={'x':'oof_x', 'y':'oof_y'})\n",
    "\n",
    "    # waypoint補正前のx,yでの評価\n",
    "    oof_score_post_process = mean_position_error(\n",
    "        oofs_df['oof_min_x'], oofs_df['oof_min_y'], 0, \n",
    "        oofs_df['target_x'], oofs_df['target_y'], 0\n",
    "        )\n",
    "    print(f\"cost-CV:{oof_score_post_process}\")\n",
    "\n",
    "    #############\n",
    "    # snap to grid\n",
    "    #############\n",
    "    train_waypoints = pd.read_csv('../../input/indoor-location-train-waypoints/train_waypoints.csv')[['x', 'y', 'site', 'floor']]\n",
    "    #auto_waypoints = pd.read_csv('../../input/auto_waypoint_v1.csv')\n",
    "    # train_waypoints = pd.concat([train_waypoints, auto_waypoints]).reset_index(drop=True)\n",
    "    snap_df = oofs_df[['site_path_timestamp','floor','oof_min_x','oof_min_y']].copy()\n",
    "    snap_df = snap_df.rename(columns={'oof_min_x':'x', 'oof_min_y':'y'})\n",
    "    snap_df = sub_process(snap_df, train_waypoints)\n",
    "    snap_df = add_xy(snap_df)\n",
    "    train_waypoints = add_xy(train_waypoints)\n",
    "\n",
    "    ds = []\n",
    "    for (site, myfloor), d in tqdm(snap_df.groupby(['site','floor'])):\n",
    "        true_floor_locs = train_waypoints.loc[(train_waypoints['floor'] == myfloor) &\n",
    "                                            (train_waypoints['site'] == site)] \\\n",
    "            .reset_index(drop=True)\n",
    "        if len(true_floor_locs) == 0:\n",
    "            print(f'Skipping {site} {myfloor}')\n",
    "            continue\n",
    "        d['matched_point'] = [closest_point(x, list(true_floor_locs['xy'])) for x in d['xy']]\n",
    "        d['x_'] = d['matched_point'].apply(lambda x: x[0])\n",
    "        d['y_'] = d['matched_point'].apply(lambda x: x[1])\n",
    "        ds.append(d)\n",
    "\n",
    "    # 上書き\n",
    "    snap_df = pd.concat(ds).sort_index()\n",
    "    # Calculate the distances\n",
    "    snap_df['dist'] = np.sqrt((snap_df['x']-snap_df['x_'])**2 + (snap_df['y']-snap_df['y_'])**2)\n",
    "    snap_df = snap_to_grid(snap_df)\n",
    "\n",
    "    # 更新後のデータを追加\n",
    "    oofs_df['oof_min_snap_x'] = snap_df['_x_']\n",
    "    oofs_df['oof_min_snap_y'] = snap_df['_y_']\n",
    "\n",
    "    # waypoint補正前のx,yでの評価\n",
    "    oof_score_post_process = mean_position_error(\n",
    "        oofs_df['oof_min_snap_x'], oofs_df['oof_min_snap_y'], 0, \n",
    "        oofs_df['target_x'], oofs_df['target_y'], 0\n",
    "        )\n",
    "    print(f\"cost-snap-CV:{oof_score_post_process}\")\n",
    "    return oofs_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 2  # 後処理の繰り返し回数\n",
    "for n in range(N):\n",
    "    if n == 0:\n",
    "        oofs_df = oofs_df.rename(columns={'x':'target_x', 'y':'target_y', 'oof_x':'x', 'oof_y':'y'})\n",
    "    else:\n",
    "        oofs_df = oofs_df.drop(['oof_x', 'oof_y', 'oof_min_x', 'oof_min_y'], axis=1).rename(columns={'oof_min_snap_x':'x', 'oof_min_snap_y':'y'})\n",
    "    oofs_df = post_processing_for_train(oofs_df, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# waypoint補正前のx,yでの評価\n",
    "oof_score_post_process = mean_position_error(\n",
    "    oofs_df['oof_min_x'], oofs_df['oof_min_y'], 0, \n",
    "    oofs_df['target_x'], oofs_df['target_y'], 0\n",
    "    )\n",
    "wandb_config['CV_cost'] = oof_score_post_process\n",
    "print(f\"(after cost-min) CV:{oof_score_post_process}\")\n",
    "\n",
    "# waypoint補正前のx,yでの評価\n",
    "oof_score_post_process = mean_position_error(\n",
    "    oofs_df['oof_min_snap_x'], oofs_df['oof_min_snap_y'], 0, \n",
    "    oofs_df['target_x'], oofs_df['target_y'], 0\n",
    "    )\n",
    "wandb_config['CV_cost_snap'] = oof_score_post_process\n",
    "print(f\"(after cost-min + snap) CV:{oof_score_post_process}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# oofs_df.to_csv(\"oof_pp.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_processing_for_test(sub, n):\n",
    "\n",
    "    sub_org = sub.copy()\n",
    "    tmp = sub['site_path_timestamp'].apply(lambda s : pd.Series(s.split('_')))\n",
    "    sub['site'] = tmp[0]\n",
    "    sub['path'] = tmp[1]\n",
    "    sub['timestamp'] = tmp[2].astype(float)\n",
    "\n",
    "    ###############\n",
    "    # cost min\n",
    "    ###############\n",
    "    processes = multiprocessing.cpu_count()\n",
    "    with multiprocessing.Pool(processes=processes) as pool:\n",
    "        dfs = pool.imap_unordered(correct_path, sub.groupby(['path']))\n",
    "        dfs = tqdm(dfs)\n",
    "        dfs = list(dfs)\n",
    "    new_sub = pd.concat(dfs).sort_values('site_path_timestamp')\n",
    "\n",
    "    ##############\n",
    "    # snap to grid\n",
    "    ##############\n",
    "    train_waypoints = pd.read_csv('../../input/indoor-location-train-waypoints/train_waypoints.csv')[['x', 'y', 'site', 'floor']]\n",
    "    # auto_waypoints = pd.read_csv('../../input/auto_waypoint_v1.csv')\n",
    "    # train_waypoints = pd.concat([train_waypoints, auto_waypoints]).reset_index(drop=True)\n",
    "    new_sub = sub_process(new_sub, train_waypoints)\n",
    "    new_sub = add_xy(new_sub)\n",
    "    train_waypoints = add_xy(train_waypoints)\n",
    "\n",
    "    ds = []\n",
    "    for (site, myfloor), d in new_sub.groupby(['site','floor']):\n",
    "        true_floor_locs = train_waypoints.loc[(train_waypoints['floor'] == myfloor) &\n",
    "                                            (train_waypoints['site'] == site)] \\\n",
    "            .reset_index(drop=True)\n",
    "        if len(true_floor_locs) == 0:\n",
    "            print(f'Skipping {site} {myfloor}')\n",
    "            continue\n",
    "        d['matched_point'] = [closest_point(x, list(true_floor_locs['xy'])) for x in d['xy']]\n",
    "        d['x_'] = d['matched_point'].apply(lambda x: x[0])\n",
    "        d['y_'] = d['matched_point'].apply(lambda x: x[1])\n",
    "        ds.append(d)\n",
    "\n",
    "    new_sub2 = pd.concat(ds)\n",
    "\n",
    "    # Calculate the distances\n",
    "    new_sub2['dist'] = np.sqrt((new_sub2['x']-new_sub2['x_'])**2 + (new_sub2['y']-new_sub2['y_'])**2)\n",
    "    new_sub2 = snap_to_grid(new_sub2)\n",
    "\n",
    "    new_sub2 = new_sub2[['site_path_timestamp','floor','_x_','_y_']].sort_index()\n",
    "    new_sub2 = new_sub2.rename(columns={'_x_':'x', '_y_':'y'})\n",
    "\n",
    "    ################\n",
    "    # device id leak\n",
    "    ################\n",
    "    new_sub3 = apply_device_leak_fast(new_sub2)\n",
    "\n",
    "    return new_sub3"
   ]
  },
  {
   "source": [
    "## device id leak pp"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_device_leak_fast(sub):\n",
    "    # 時間短縮\n",
    "    leak_sub = sub.copy()\n",
    "    leak_id = np.load('../exp114/leak_id.npy')\n",
    "    _leak_sub = pd.read_csv('../exp114/exp114_cost_snap_leak_sub.csv')\n",
    "    leak_sub.loc[leak_id, 'x'] = _leak_sub.loc[leak_id, 'x']\n",
    "    leak_sub.loc[leak_id, 'y'] = _leak_sub.loc[leak_id, 'y']\n",
    "    print(((leak_sub['x'].to_numpy() - sub['x'].to_numpy())!=0).sum())  # 変更されたデータ数\n",
    "    print(((leak_sub['y'].to_numpy() - sub['y'].to_numpy())!=0).sum())  # 変更されたデータ数\n",
    "    return leak_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 6\n",
    "sub = sub.reset_index()\n",
    "for n in range(N):\n",
    "    sub = post_processing_for_test(sub, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(EXP_NAME + '_cost_snap_leak_sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  },
  {
   "source": [
    "## sub確認"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(EXP_NAME + '_cost_snap_leak_sub.csv')\n",
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_sub = pd.read_csv(\"../exp060/exp060_cost_snap_sub.csv\")\n",
    "_x = compare_sub[\"x\"]\n",
    "_y = compare_sub[\"y\"]\n",
    "x = sub[\"x\"]\n",
    "y = sub[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, _x, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y, _y, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}